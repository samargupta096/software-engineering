# Elasticsearch Internals, Architecture & Best Practices

> ğŸ”§ **Level: Advanced** | â±ï¸ **Reading Time: 60 min** | ğŸ”— **[â† Learning Hub](./elasticsearch-deep-dive.md)** | **[â† Research Guide](./elasticsearch-research.md)**

---

## ğŸ—ºï¸ Quick Navigation

| Section | What You'll Learn |
|---------|-------------------|
| [Internal Architecture](#internal-architecture) | Layered design, Node structure, Memory layout |
| [How ES Works](#how-elasticsearch-works-under-the-hood) | Document lifecycle, Transaction logs |
| [Lucene Integration](#lucene-integration) | Segments, Inverted index internals |
| [Shard Internals](#shard-internals) | Sizing, Allocation, Rebalancing |
| [Search Execution](#search-execution) | Query phases, Caching, BM25 scoring |
| [Memory Management](#memory--storage-management) | Heap vs Off-heap, GC tuning |
| [Scaling Patterns](#scaling-patterns) | Hot-Warm-Cold architecture |
| [Best Practices](#best-practices) | Production checklist |

> [!IMPORTANT]
> **Prerequisites:** Understanding of [Elasticsearch basics](./elasticsearch-research.md) is strongly recommended before diving into internals.

---

## Table of Contents

1. [Internal Architecture](#internal-architecture)
2. [How Elasticsearch Works Under the Hood](#how-elasticsearch-works-under-the-hood)
3. [Lucene Integration](#lucene-integration)
4. [Shard Internals](#shard-internals)
5. [Indexing Pipeline](#indexing-pipeline)
6. [Search Execution](#search-execution)
7. [Memory & Storage Management](#memory--storage-management)
8. [Cluster Coordination](#cluster-coordination)
9. [Scaling Patterns](#scaling-patterns)
10. [Best Practices](#best-practices)
11. [Common Pitfalls & Solutions](#common-pitfalls--solutions)

---

## Internal Architecture


### The Layered Architecture

```mermaid
graph TB
    subgraph Client["ğŸ“± Client Layer"]
        REST["REST API"]
        Transport["Transport Protocol"]
        Clients["Native Clients"]
    end
    
    subgraph ES["âš¡ Elasticsearch Layer"]
        direction LR
        subgraph Services["Core Services"]
            Routing["Routing Module"]
            Aggregation["Aggregation Engine"]
            Query["Query Parser"]
        end
        subgraph Management["Management"]
            Cluster["Cluster Service"]
            IndexSvc["Index Service"]
            ShardSvc["Shard Service"]
        end
    end
    
    subgraph Lucene["ğŸ” Lucene Layer"]
        direction LR
        InvertedIdx["Inverted Index"]
        StoredFields["Stored Fields"]
        DocValues["Doc Values"]
        Segments["Segments"]
    end
    
    subgraph Storage["ğŸ’¾ Storage Layer"]
        FS["File System"]
        Disk["SSD/HDD"]
    end
    
    Client --> ES
    ES --> Lucene
    Lucene --> Storage
    
    style Client fill:#E3F2FD
    style ES fill:#FFF3E0
    style Lucene fill:#E8F5E9
    style Storage fill:#F3E5F5
```

<details>
<summary>ğŸ“ ASCII Version (click to expand)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CLIENT LAYER                             â”‚
â”‚  (REST API, Transport Protocol, Native Clients)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ELASTICSEARCH LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Routing    â”‚ â”‚  Aggregation â”‚ â”‚    Query     â”‚        â”‚
â”‚  â”‚   Module     â”‚ â”‚    Engine    â”‚ â”‚    Parser    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Cluster    â”‚ â”‚    Index     â”‚ â”‚    Shard     â”‚        â”‚
â”‚  â”‚   Service    â”‚ â”‚   Service    â”‚ â”‚   Service    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LUCENE LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Inverted   â”‚ â”‚    Stored    â”‚ â”‚   Doc Values â”‚        â”‚
â”‚  â”‚    Index     â”‚ â”‚    Fields    â”‚ â”‚              â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Segments   â”‚ â”‚    Codec     â”‚ â”‚  Directory   â”‚        â”‚
â”‚  â”‚              â”‚ â”‚              â”‚ â”‚              â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STORAGE LAYER                               â”‚
â”‚         (File System, SSD/HDD, Network Storage)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>


### Node Architecture Deep Dive

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ELASTICSEARCH NODE                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    JVM HEAP                             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Query Cache â”‚  â”‚  Field Data  â”‚  â”‚   Request   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚    Cache     â”‚  â”‚   Cache     â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Index Bufferâ”‚  â”‚  Thread Poolsâ”‚  â”‚   Network   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚   Buffers   â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  FILE SYSTEM CACHE                      â”‚ â”‚
â”‚  â”‚           (OS-managed, uses remaining RAM)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     DISK STORAGE                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚ â”‚
â”‚  â”‚  â”‚ Segments â”‚  â”‚Transactionâ”‚ â”‚  Commit  â”‚             â”‚ â”‚
â”‚  â”‚  â”‚  (.cfs)  â”‚  â”‚ Log (wal) â”‚ â”‚  Point   â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Breakdown (Typical 64GB Server)

```
Total RAM: 64 GB
â”‚
â”œâ”€ JVM Heap: 31 GB (50% of RAM, max 32GB due to compressed OOPs)
â”‚  â”œâ”€ Old Generation: ~24 GB
â”‚  â”œâ”€ Young Generation: ~7 GB
â”‚  â””â”€ Caches:
â”‚     â”œâ”€ Query Cache: ~10% of heap
â”‚     â”œâ”€ Field Data Cache: ~40% of heap
â”‚     â””â”€ Index Buffer: 10% of heap (default)
â”‚
â””â”€ OS File System Cache: 33 GB (50% of RAM)
   â””â”€ Used for caching Lucene segments
```

**Why 31GB and not 32GB for heap?**
- Java uses "Compressed OOPs" (Ordinary Object Pointers) below 32GB
- Crossing 32GB disables compression â†’ effective memory decreases
- Sweet spot: 31GB heap, rest for OS cache

---

## How Elasticsearch Works Under the Hood

### Document Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Indexed: POST /index/_doc
    
    state "In Memory" as Memory {
        Indexed --> TransactionLog: Write to disk
        TransactionLog --> MemoryBuffer: Index in buffer
    }
    
    state "Near Real-Time" as NRT {
        MemoryBuffer --> Segment: Refresh (1s)
        Segment --> Searchable: Added to search path
    }
    
    state "Durable" as Durable {
        Searchable --> Flushed: Flush (30m)
        Flushed --> CommitPoint: fsync to disk
    }
    
    CommitPoint --> [*]: Transaction log cleared
    
    note right of TransactionLog
        Durability guarantee
        Used for crash recovery
    end note
    
    note right of Segment
        Documents become
        searchable here
    end note
```

> ğŸ“Œ **Key Timing:** Refresh = ~1s (searchable) | Flush = ~30min (durable)

<details>
<summary>ğŸ“ Full document lifecycle details (click to expand)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CLIENT SENDS DOCUMENT                                    â”‚
â”‚     POST /products/_doc/123                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. COORDINATING NODE                                        â”‚
â”‚     - Receives request                                       â”‚
â”‚     - Determines routing: hash(_id) % num_primary_shards     â”‚
â”‚     - Identifies primary shard location                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. PRIMARY SHARD (Node 2)                                   â”‚
â”‚     a) Validates document                                    â”‚
â”‚     b) Writes to Transaction Log (fsync to disk)             â”‚
â”‚     c) Indexes into Lucene in-memory buffer                  â”‚
â”‚     d) Forwards to replica shards (parallel)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. REPLICA SHARDS (Node 1, Node 3)                          â”‚
â”‚     - Write to transaction log                               â”‚
â”‚     - Index into Lucene buffer                               â”‚
â”‚     - Acknowledge to primary                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. PRIMARY SHARD                                            â”‚
â”‚     - Waits for quorum (configurable)                        â”‚
â”‚     - Returns success to coordinating node                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. REFRESH CYCLE (default: every 1 second)                  â”‚
â”‚     - In-memory buffer â†’ new Lucene segment                  â”‚
â”‚     - Segment written to file system cache                   â”‚
â”‚     - Document becomes searchable (near real-time)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. FLUSH CYCLE (default: every 30 min or 512MB translog)   â”‚
â”‚     - All segments fsync'd to disk                           â”‚
â”‚     - Transaction log truncated                              â”‚
â”‚     - Commit point created                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>


### Transaction Log (Write-Ahead Log)

**Purpose**: Durability & crash recovery

```
Time    Action                      Memory          Disk
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T0      Index doc 1                 Buffer          Translog âœ“
T1      Index doc 2                 Buffer          Translog âœ“
T2      Index doc 3                 Buffer          Translog âœ“
T3      Refresh (1s)                â†’ Segment       Translog âœ“
                                    (searchable)
T4      Index doc 4                 Buffer          Translog âœ“
T5      Index doc 5                 Buffer          Translog âœ“
T6      Refresh (1s)                â†’ Segment       Translog âœ“
T7      Flush (30m)                 â†’ Fsync all     Translog cleared
                                      segments
```

**On Crash:**
1. Elasticsearch reads last commit point
2. Replays transaction log from last commit
3. Recovers all indexed but unflushed documents

---

## Lucene Integration

### What is Lucene?

Lucene is the **core search library** that Elasticsearch wraps. Understanding Lucene is key to understanding Elasticsearch internals.

```
Elasticsearch Index
â”œâ”€ Shard 0 (Lucene Index)
â”‚  â”œâ”€ Segment 1
â”‚  â”œâ”€ Segment 2
â”‚  â””â”€ Segment 3
â”œâ”€ Shard 1 (Lucene Index)
â”‚  â”œâ”€ Segment 1
â”‚  â””â”€ Segment 2
â””â”€ Shard 2 (Lucene Index)
   â”œâ”€ Segment 1
   â””â”€ Segment 2
```

**Key Point**: Each Elasticsearch shard = one Lucene index

### Lucene Segment Structure

```
Segment_1/
â”œâ”€ _0.cfs         # Compound file (contains all below)
â”œâ”€ _0.cfe         # Compound file entries
â”‚
â”œâ”€ Inverted Index # Term â†’ Document mapping
â”‚  â”œâ”€ .tim        # Term dictionary
â”‚  â”œâ”€ .tip        # Term index (pointer to .tim)
â”‚  â””â”€ .doc/.pos   # Document/position data
â”‚
â”œâ”€ Stored Fields  # Original document storage
â”‚  â”œâ”€ .fdt        # Field data
â”‚  â””â”€ .fdx        # Field index
â”‚
â”œâ”€ Doc Values     # Column-oriented storage
â”‚  â”œâ”€ .dvd        # Doc values data
â”‚  â””â”€ .dvm        # Doc values metadata
â”‚
â”œâ”€ Norms          # Field length normalization
â”œâ”€ Term Vectors   # Per-document term statistics
â””â”€ Deleted Docs   # .liv (live docs - deleted bitmap)
```

### Inverted Index Deep Dive

**Example Documents:**
```
Doc 1: "Elasticsearch is fast"
Doc 2: "Lucene is powerful"
Doc 3: "Elasticsearch uses Lucene"
```

**After Analysis (lowercase, tokenize):**
```
Term           â†’ Documents   Positions
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
elasticsearch  â†’ [1, 3]      [1:0, 3:0]
fast           â†’ [1]         [1:2]
is             â†’ [1, 2]      [1:1, 2:1]
lucene         â†’ [2, 3]      [2:0, 3:2]
powerful       â†’ [2]         [2:2]
uses           â†’ [3]         [3:1]
```

**Additional Data Structures:**

1. **Term Dictionary** (sorted)
   ```
   elasticsearch â†’ pointer to posting list
   fast â†’ pointer to posting list
   ...
   ```

2. **Skip Lists** (for fast search)
   ```
   If term appears in 10,000 docs, skip list allows jumping:
   Doc 1 â†’ Doc 100 â†’ Doc 500 â†’ Doc 1000 â†’ ...
   ```

3. **Doc Values** (columnar storage for sorting/aggregations)
   ```
   Doc ID    price    category
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1         999.99   electronics
   2         29.99    accessories
   3         1299.99  electronics
   ```

### Segment Merging

**Problem**: Over time, many small segments accumulate

```
Initial State:
Segment_1 (1000 docs)
Segment_2 (500 docs)
Segment_3 (200 docs)
Segment_4 (100 docs)
Segment_5 (50 docs)

After Merge:
Segment_6 (1850 docs)  â† Merged 1,2,3,4,5
```

**Merge Policy** (TieredMergePolicy - default):
- Merges segments of similar sizes
- Limits max segment size (5GB default)
- Runs in background
- Deletes old segments after merge

**Benefits:**
- Fewer segments â†’ faster searches
- Deleted documents purged during merge
- Better compression

**Cost:**
- I/O intensive
- CPU usage
- Temporary disk space (2x segment size)

---

## Shard Internals

### Shard = Lucene Index

```
Primary Shard 0 (on Node 1)
â”‚
â”œâ”€ Transaction Log
â”‚  â”œâ”€ translog-1.tlog
â”‚  â””â”€ translog-2.tlog
â”‚
â”œâ”€ Segments (Lucene)
â”‚  â”œâ”€ _0.cfs (100 docs)
â”‚  â”œâ”€ _1.cfs (200 docs)
â”‚  â”œâ”€ _2.cfs (500 docs)
â”‚  â””â”€ _3.cfs (1000 docs)
â”‚
â”œâ”€ Commit Point
â”‚  â””â”€ segments_N (points to active segments)
â”‚
â””â”€ In-Memory Structures
   â”œâ”€ Index Buffer (for new docs)
   â”œâ”€ Query Cache
   â””â”€ Field Data Cache
```

### Why Shards?

1. **Horizontal Scaling**: Distribute data across nodes
2. **Parallelization**: Query all shards simultaneously
3. **Fault Tolerance**: With replicas, can lose nodes

### Shard Sizing Rules

**Over-Sharding Problems:**
- Each shard has overhead (file handles, memory)
- Too many shards â†’ cluster state bloat
- Small shards â†’ merge overhead

**Under-Sharding Problems:**
- Can't distribute load
- Large shards â†’ slow recovery
- Limited parallelism

**Best Practice Formula:**
```
Shard Size: 20-40 GB (sweet spot)
Max Shard Size: 50 GB

Example:
- Expected index size: 300 GB
- Shards needed: 300 / 30 = 10 shards
- With 1 replica: 20 shards total
```

### Shard Allocation & Rebalancing

**Allocation Decisions:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARD ALLOCATION DECISION PROCESS                           â”‚
â”‚                                                              â”‚
â”‚  1. Allocation Filters (user-defined)                        â”‚
â”‚     - Force shards to specific nodes                         â”‚
â”‚     - Exclude certain nodes                                  â”‚
â”‚                                                              â”‚
â”‚  2. Allocation Awareness (rack/zone)                         â”‚
â”‚     - Don't put primary + replica in same rack               â”‚
â”‚                                                              â”‚
â”‚  3. Disk Watermarks                                          â”‚
â”‚     - Low (85%): Stop allocating to node                     â”‚
â”‚     - High (90%): Move shards away                           â”‚
â”‚     - Flood (95%): Read-only block                           â”‚
â”‚                                                              â”‚
â”‚  4. Same-Shard Allocation                                    â”‚
â”‚     - Don't put primary + replica on same node               â”‚
â”‚                                                              â”‚
â”‚  5. Load Balancing                                           â”‚
â”‚     - Distribute shards evenly                               â”‚
â”‚     - Consider node resources                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Rebalancing Example:**
```
Initial State (3 nodes):
Node 1: [P0, P1, R2]  (3 shards)
Node 2: [P2, R0]      (2 shards)
Node 3: [R1]          (1 shard)

After Rebalancing:
Node 1: [P0, R2]      (2 shards)
Node 2: [P1, R0]      (2 shards)
Node 3: [P2, R1]      (2 shards)
```

---

## Indexing Pipeline

### Full Indexing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. INGEST PIPELINE (Optional)                               â”‚
â”‚     - Processors: grok, date, geoip, script, etc.            â”‚
â”‚     - Transform/enrich document before indexing              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. MAPPING & ANALYSIS                                       â”‚
â”‚     - Apply mapping (explicit or dynamic)                    â”‚
â”‚     - Run analyzers on text fields                           â”‚
â”‚     - Generate tokens                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. LUCENE INDEXING                                          â”‚
â”‚     - Create inverted index entries                          â”‚
â”‚     - Store original fields                                  â”‚
â”‚     - Generate doc values                                    â”‚
â”‚     - Update in-memory buffer                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. REFRESH (make searchable)                                â”‚
â”‚     - In-memory buffer â†’ new segment                         â”‚
â”‚     - Segment in filesystem cache                            â”‚
â”‚     - Segment added to search path                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. MERGE (background)                                       â”‚
â”‚     - Small segments merged into larger ones                 â”‚
â”‚     - Deleted docs removed                                   â”‚
â”‚     - Old segments deleted                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Analysis Chain

**Example: Analyzing "The Quick BROWN fox!"**

```
Original Text: "The Quick BROWN fox!"
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Char Filter        â”‚  (HTML strip, pattern replace)
â”‚  (none in example)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
"The Quick BROWN fox!"
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tokenizer          â”‚  (standard, whitespace, etc.)
â”‚  (standard)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
["The", "Quick", "BROWN", "fox"]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Filters      â”‚  (lowercase, stop, stemmer, etc.)
â”‚  (lowercase)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
["the", "quick", "brown", "fox"]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Filters      â”‚
â”‚  (stop filter)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
["quick", "brown", "fox"]  (removed "the")
        â†“
Final Tokens: ["quick", "brown", "fox"]
```

**Custom Analyzer Example:**
```json
PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": ["html_strip"],
          "tokenizer": "standard",
          "filter": ["lowercase", "stop", "snowball"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "content": {
        "type": "text",
        "analyzer": "my_custom_analyzer"
      }
    }
  }
}
```

### Bulk Indexing Internals

**Single vs Bulk:**
```
Single Indexing (1000 docs):
Request 1 â†’ Network â†’ Node â†’ Shard â†’ Lucene
Request 2 â†’ Network â†’ Node â†’ Shard â†’ Lucene
...
Request 1000 â†’ Network â†’ Node â†’ Shard â†’ Lucene

Network round trips: 1000
Per-document overhead: High

Bulk Indexing (1000 docs):
Bulk Request (1000 docs) â†’ Network â†’ Node
Node splits by shard â†’ Parallel processing
Shard 0: [doc1, doc2, ...]
Shard 1: [doc3, doc4, ...]
...

Network round trips: 1
Batch processing: Efficient
```

**Optimal Bulk Size:**
- Start with 5-15 MB per batch
- Monitor queue rejections
- Adjust based on hardware

---

## Search Execution

### Query Execution Phases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: QUERY PHASE (scatter)                              â”‚
â”‚                                                              â”‚
â”‚  Coordinating Node                                           â”‚
â”‚       â†“                                                      â”‚
â”‚  Broadcasts query to all shards                              â”‚
â”‚       â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Shard 0  â”‚  â”‚ Shard 1  â”‚  â”‚ Shard 2  â”‚                  â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚                  â”‚
â”‚  â”‚ Executes â”‚  â”‚ Executes â”‚  â”‚ Executes â”‚                  â”‚
â”‚  â”‚ query    â”‚  â”‚ query    â”‚  â”‚ query    â”‚                  â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚                  â”‚
â”‚  â”‚ Returns: â”‚  â”‚ Returns: â”‚  â”‚ Returns: â”‚                  â”‚
â”‚  â”‚ Doc IDs  â”‚  â”‚ Doc IDs  â”‚  â”‚ Doc IDs  â”‚                  â”‚
â”‚  â”‚ + Scores â”‚  â”‚ + Scores â”‚  â”‚ + Scores â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚       â†“              â†“              â†“                        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                      â†“                                       â”‚
â”‚         Coordinating Node merges results                     â”‚
â”‚         Sorts by score, takes top N                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: FETCH PHASE (gather)                               â”‚
â”‚                                                              â”‚
â”‚  Coordinating Node identifies which shards have results      â”‚
â”‚       â†“                                                      â”‚
â”‚  Requests full documents from those shards                   â”‚
â”‚       â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ Shard 0  â”‚  â”‚ Shard 2  â”‚                                â”‚
â”‚  â”‚          â”‚  â”‚          â”‚                                â”‚
â”‚  â”‚ Returns  â”‚  â”‚ Returns  â”‚                                â”‚
â”‚  â”‚ Doc 1    â”‚  â”‚ Doc 5    â”‚                                â”‚
â”‚  â”‚ Doc 3    â”‚  â”‚ Doc 8    â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚       â†“              â†“                                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚              â†“                                               â”‚
â”‚  Coordinating Node assembles final response                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Optimization

**1. Query Cache (Node Level)**
```
Cache Key: Hash(query + filter + shard)
Cache Size: 10% of heap (default)
Invalidation: On refresh or delete

Example:
First request: {"term": {"status": "active"}}
â†’ Executes query â†’ Caches result (doc IDs)

Second request: Same query
â†’ Returns cached doc IDs (instant)
```

**2. Field Data Cache (Heap)**
```
Used for: Sorting, aggregations on text fields
Storage: Loaded per-field, per-segment
Warning: Can cause OOM if not careful

Example:
GET /logs/_search
{
  "aggs": {
    "top_users": {
      "terms": { "field": "username.keyword" }  â† Loads into field data
    }
  }
}
```

**Better Alternative: Doc Values**
```
Doc values: Column-oriented, on-disk storage
Used for: Sorting, aggregations
Memory: Loaded on-demand, OS cache
Performance: Nearly as fast, much safer

Mapping:
{
  "username": {
    "type": "keyword"  â† Doc values enabled by default
  }
}
```

**3. Request Cache (Node Level)**
```
Caches: Entire response (size=0 queries, aggregations)
Key: Hash(entire request + index)
Size: 1% of heap (default)

Use case: Dashboards, repeated aggregations
```

### Scoring & Relevance

**TF-IDF (Classic Scoring)**
```
score(doc, query) = queryNorm Ã— coord Ã— Î£(tf Ã— idf Ã— boost Ã— norm)

Where:
- TF (Term Frequency): How often term appears in document
- IDF (Inverse Document Frequency): Rarity of term across all docs
- Norm: Field length normalization
- Boost: User-defined boost
```

**BM25 (Default in ES 5.0+)**
```
score(doc, query) = IDF Ã— (TF Ã— (k1 + 1)) / (TF + k1 Ã— (1 - b + b Ã— (fieldLen / avgFieldLen)))

Where:
- k1: Controls term frequency saturation (default 1.2)
- b: Controls field length normalization (default 0.75)

Better than TF-IDF:
- Diminishing returns for term frequency
- Better field length normalization
```

**Example:**
```
Document 1: "cat cat cat" (3x "cat")
Document 2: "cat" (1x "cat")

TF-IDF: Doc 1 scores much higher (3x term frequency)
BM25: Doc 1 scores slightly higher (saturation effect)
```

---

## Memory & Storage Management

### Heap vs Off-Heap Memory

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JVM HEAP (31GB)                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  OLD GENERATION (~24GB)                                      â”‚
â”‚  â”œâ”€ Query Cache (10% = ~3GB)                                â”‚
â”‚  â”œâ”€ Field Data Cache (40% = ~12GB)                          â”‚
â”‚  â”œâ”€ Index Buffer (10% = ~3GB)                               â”‚
â”‚  â”œâ”€ Request Cache (1% = ~300MB)                             â”‚
â”‚  â””â”€ Other objects (segments metadata, cluster state, etc.)  â”‚
â”‚                                                              â”‚
â”‚  YOUNG GENERATION (~7GB)                                     â”‚
â”‚  â””â”€ Short-lived objects (requests, responses, temp data)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ GC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OFF-HEAP (OS File System Cache ~33GB)                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  â”œâ”€ Lucene Segments (mmap'd files)                          â”‚
â”‚  â”œâ”€ Doc Values                                              â”‚
â”‚  â”œâ”€ Term Dictionary                                         â”‚
â”‚  â””â”€ Stored Fields                                           â”‚
â”‚                                                              â”‚
â”‚  Benefits:                                                   â”‚
â”‚  - No GC overhead                                            â”‚
â”‚  - OS manages caching intelligently                          â”‚
â”‚  - Can use all available RAM                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Garbage Collection

**G1GC (Default in ES 7.0+)**
```
Young GC (frequent, <100ms):
- Clears young generation
- Promotes survivors to old generation

Mixed GC (periodic):
- Clears young + portions of old generation
- Goal: Keep pauses low

Full GC (rare, BAD):
- Stop-the-world
- Can take seconds/minutes
- Sign of heap pressure
```

**GC Tuning:**
```
# elasticsearch.yml or jvm.options
-Xms31g
-Xmx31g
-XX:+UseG1GC
-XX:MaxGCPauseMillis=200
-XX:InitiatingHeapOccupancyPercent=75

Monitor:
GET /_nodes/stats/jvm
```

### Disk I/O Optimization

**1. Use SSDs**
```
HDD vs SSD for Elasticsearch:

HDD (7200 RPM):
- Random IOPS: ~100
- Sequential: ~150 MB/s
- Merge storms: Painful
- Search latency: High

SSD (NVMe):
- Random IOPS: ~500,000
- Sequential: ~3,500 MB/s
- Merge storms: Handled easily
- Search latency: Low

Elasticsearch is I/O bound â†’ SSDs = massive improvement
```

**2. RAID Configuration**
```
Don't use RAID for Elasticsearch:
- Elasticsearch provides replication
- RAID adds complexity
- RAID rebuild time is high
- Use replicas instead (faster recovery)

Exception: RAID 0 for temp/cache (if you're crazy)
```

**3. File System**
```
Recommended: ext4 or XFS
- Good performance
- Stable
- Well-tested with Elasticsearch

Avoid:
- NFS (network overhead)
- Windows NTFS (slower)
```

---

## Cluster Coordination

### Master Node Election

**Zen Discovery (ES < 7.0):**
```
1. Nodes vote for master
2. Majority wins (quorum)
3. Minimum master nodes = (total_nodes / 2) + 1

Problem: Split-brain if misconfigured
```

**Voting Configuration (ES 7.0+):**
```
Auto-managed quorum:
- No manual minimum_master_nodes
- Cluster auto-adjusts voting configuration
- Safer, prevents split-brain

Election Process:
1. Master candidate broadcasts: "I want to be master"
2. Nodes vote
3. Majority wins
4. New master publishes cluster state
```

### Cluster State

**What is Cluster State?**
```json
{
  "cluster_name": "my-cluster",
  "version": 42,
  "master_node": "node-1",
  "nodes": {
    "node-1": { "name": "node-1", "roles": ["master", "data"] },
    "node-2": { "name": "node-2", "roles": ["data"] }
  },
  "metadata": {
    "indices": {
      "products": {
        "settings": { "number_of_shards": 5 },
        "mappings": { ... },
        "aliases": { ... }
      }
    },
    "templates": { ... }
  },
  "routing_table": {
    "indices": {
      "products": {
        "shards": {
          "0": [
            { "state": "STARTED", "primary": true, "node": "node-1" },
            { "state": "STARTED", "primary": false, "node": "node-2" }
          ]
        }
      }
    }
  }
}
```

**Cluster State Updates:**
```
1. Master receives change (e.g., create index)
2. Master updates cluster state (in-memory)
3. Master publishes new state to all nodes
4. Nodes acknowledge
5. Change committed
```

**Problem: Large Cluster State**
```
Symptoms:
- Slow cluster state updates
- High master CPU
- Delayed index creation

Causes:
- Too many indices (thousands)
- Too many shards (tens of thousands)
- Large mappings

Solution:
- Use index lifecycle management
- Reduce shard count
- Use rollover for time-series data
```

---

## Scaling Patterns

### Horizontal Scaling (Add Nodes)

**Scaling Data Nodes:**
```
Before (3 nodes, 9 shards):
Node 1: [P0, P1, P2]
Node 2: [R0, R1, R2]
Node 3: [R0, R1, R2]

Add Node 4:
Elasticsearch rebalances automatically

After (4 nodes, 9 shards):
Node 1: [P0, R1]
Node 2: [P1, R2]
Node 3: [P2, R0]
Node 4: [R0, R1, R2]

Benefits:
- More RAM for caching
- More CPU for queries
- Better distribution
```

### Vertical Scaling (Bigger Nodes)

```
Option 1: Many small nodes
- 10 nodes Ã— 16GB RAM = 160GB total
- Pros: Better fault tolerance
- Cons: More network overhead

Option 2: Few large nodes
- 3 nodes Ã— 64GB RAM = 192GB total
- Pros: Less overhead, better for aggregations
- Cons: Longer recovery time if node fails

Recommendation: Balance based on use case
- Logs/time-series: More small nodes
- Analytics: Fewer large nodes
```

### Hot-Warm-Cold Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOT TIER (Fast writes, recent data)                       â”‚
â”‚  â”œâ”€ High-performance SSDs                                  â”‚
â”‚  â”œâ”€ More CPU/RAM                                           â”‚
â”‚  â”œâ”€ Active indexing + searching                            â”‚
â”‚  â””â”€ Example: Last 7 days of logs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (ILM policy)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WARM TIER (Read-only, older data)                         â”‚
â”‚  â”œâ”€ Standard SSDs                                          â”‚
â”‚  â”œâ”€ Moderate resources                                     â”‚
â”‚  â”œâ”€ Searching only (no indexing)                           â”‚
â”‚  â””â”€ Example: 7-30 days of logs                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (ILM policy)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COLD TIER (Archival, rarely accessed)                     â”‚
â”‚  â”œâ”€ HDDs or cheap storage                                  â”‚
â”‚  â”œâ”€ Low resources                                          â”‚
â”‚  â”œâ”€ Infrequent searches                                    â”‚
â”‚  â””â”€ Example: 30+ days of logs                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (ILM policy)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FROZEN TIER (ES 7.12+) or Delete                          â”‚
â”‚  â””â”€ Searchable snapshots, minimal cost                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration:**
```yaml
# Hot node
node.attr.data: hot
node.roles: [data_hot, ingest]

# Warm node
node.attr.data: warm
node.roles: [data_warm]

# Cold node
node.attr.data: cold
node.roles: [data_cold]
```

**ILM Policy:**
```json
PUT /_ilm/policy/logs_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "7d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "allocate": {
            "require": { "data": "warm" }
          },
          "forcemerge": { "max_num_segments": 1 },
          "shrink": { "number_of_shards": 1 }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "allocate": {
            "require": { "data": "cold" }
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

---

## Best Practices

### 1. Index Design

**Do:**
âœ… Use time-based indices for logs (e.g., `logs-2024-01-15`)
âœ… Set shard count based on data size (20-40GB per shard)
âœ… Use explicit mappings (avoid dynamic mapping in prod)
âœ… Disable `_source` if you don't need original document
âœ… Use `keyword` for exact matches, `text` for full-text

**Don't:**
âŒ Create too many shards (overhead increases)
âŒ Use default field names (can't change later)
âŒ Index data you don't need
âŒ Use wildcard queries on large datasets
âŒ Forget to set replicas (data loss risk)

### 2. Query Optimization

**Do:**
âœ… Use `filter` context (cacheable, faster)
```json
{
  "bool": {
    "filter": [
      { "term": { "status": "active" } }
    ]
  }
}
```

âœ… Limit `_source` fields
```json
{
  "_source": ["name", "price"],
  "query": { "match_all": {} }
}
```

âœ… Use `search_after` for deep pagination
âœ… Pre-filter with `range` before `match`

**Don't:**
âŒ Use `from` + `size` for deep pagination (slow)
âŒ Use wildcard/prefix queries without limits
âŒ Load large field data into heap
âŒ Use `script` queries in hot paths (slow)

### 3. Indexing Performance

**Do:**
âœ… Use bulk API (5-15MB batches)
âœ… Disable refresh during bulk indexing
```bash
PUT /my_index/_settings
{ "index.refresh_interval": "-1" }
# After bulk indexing
PUT /my_index/_settings
{ "index.refresh_interval": "1s" }
```

âœ… Increase indexing buffer
```yaml
indices.memory.index_buffer_size: 20%
```

âœ… Use auto-generated IDs (faster than custom IDs)
âœ… Use multiple threads/clients

**Don't:**
âŒ Index one document at a time
âŒ Use small bulk sizes (overhead)
âŒ Refresh after every insert
âŒ Over-replicate (slows down indexing)

### 4. Hardware & JVM

**Do:**
âœ… Use SSDs (massive performance boost)
âœ… Set heap to 50% of RAM (max 31GB)
```yaml
-Xms31g
-Xmx31g
```

âœ… Disable swap
```bash
sudo swapoff -a
```

âœ… Use G1GC (default in ES 7.0+)
âœ… Monitor GC logs

**Don't:**
âŒ Set heap > 32GB (compressed OOPs disabled)
âŒ Use all RAM for heap (OS cache is critical)
âŒ Run other apps on ES nodes
âŒ Use spinning disks in production

### 5. Monitoring

**Key Metrics:**
```
Cluster Health:
- Status (green/yellow/red)
- Active shards
- Unassigned shards

Node Metrics:
- CPU usage
- Heap usage (< 75%)
- GC frequency/duration
- Disk usage

Index Metrics:
- Indexing rate (docs/sec)
- Search rate (queries/sec)
- Search latency (ms)
- Merge times

JVM:
- Old gen usage
- GC pauses (< 200ms)
- Thread pool rejections
```

**Tools:**
- Kibana Monitoring
- Elasticsearch Metrics API
- Prometheus + Grafana
- Marvel/X-Pack

### 6. Security

**Do:**
âœ… Enable authentication (X-Pack Security)
âœ… Use HTTPS (TLS/SSL)
âœ… Implement role-based access control
âœ… Audit logging
âœ… Network isolation (firewall)
âœ… Encrypt data at rest

**Don't:**
âŒ Expose Elasticsearch to internet without auth
âŒ Use default passwords
âŒ Give write access to read-only users
âŒ Store sensitive data unencrypted

---

## Common Pitfalls & Solutions

### 1. Out of Memory (OOM)

**Symptoms:**
- GC thrashing (constant garbage collection)
- Heap usage > 90%
- Node unresponsive
- `OutOfMemoryError`

**Causes:**
- Field data cache exhaustion
- Too many shards
- Large aggregations
- Memory leaks

**Solutions:**
```bash
# Limit field data cache
PUT /_cluster/settings
{
  "transient": {
    "indices.breaker.fielddata.limit": "40%"
  }
}

# Clear field data cache
POST /_cache/clear?fielddata=true

# Use doc values instead of field data
# (enabled by default on keyword fields)

# Reduce shard count
# Increase heap (up to 31GB)
# Use `search_after` instead of `from`/`size`
```

### 2. Slow Searches

**Causes:**
- Too many shards
- Large result sets
- Heavy aggregations
- Unoptimized queries
- Cold OS cache

**Solutions:**
```bash
# Profile queries
GET /my_index/_search
{
  "profile": true,
  "query": { ... }
}

# Use filters (cacheable)
# Reduce shard count
# Use routing
# Warm up OS cache
# Force merge (read-only indices)
POST /my_index/_forcemerge?max_num_segments=1
```

### 3. Unassigned Shards

**Causes:**
- Not enough nodes for replicas
- Disk watermark exceeded
- Shard allocation disabled
- Node excluded by allocation filtering

**Solutions:**
```bash
# Check allocation explanation
GET /_cluster/allocation/explain

# Reduce replicas
PUT /my_index/_settings
{
  "number_of_replicas": 0
}

# Increase disk watermark
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.disk.watermark.low": "90%",
    "cluster.routing.allocation.disk.watermark.high": "95%"
  }
}

# Retry allocation
POST /_cluster/reroute?retry_failed=true
```

### 4. Split Brain (Cluster Partitions)

**Cause:**
- Network partition
- Master node failure
- Misconfigured minimum master nodes (ES < 7.0)

**Prevention (ES 7.0+):**
- Auto-managed voting configuration
- No manual configuration needed

**Prevention (ES < 7.0):**
```yaml
# Set minimum master nodes
discovery.zen.minimum_master_nodes: 2  # For 3 master-eligible nodes
```

**Recovery:**
- Fix network issues
- Restart affected nodes
- May need to rebuild cluster

### 5. High Merge Activity

**Symptoms:**
- High disk I/O
- Slow indexing
- CPU spikes

**Cause:**
- Too many small segments
- Heavy indexing

**Solutions:**
```bash
# Reduce merge pressure
PUT /my_index/_settings
{
  "index.merge.scheduler.max_thread_count": 1
}

# For read-only indices, force merge
POST /my_index/_forcemerge?max_num_segments=1

# Use SSDs (handles merges better)
```

---

## Production Checklist

### Pre-Deployment

- [ ] Hardware: SSDs, sufficient RAM (64GB+)
- [ ] JVM: Heap = 50% RAM, max 31GB
- [ ] Disable swap: `swapoff -a`
- [ ] Set file descriptors: 65535+
- [ ] Network: Low latency between nodes
- [ ] Security: Authentication, HTTPS, firewall
- [ ] Backups: Snapshot repository configured

### Cluster Configuration

- [ ] Cluster name set
- [ ] Node roles defined (master, data, ingest)
- [ ] Discovery configured (seed hosts)
- [ ] Shard count planned (20-40GB per shard)
- [ ] Replicas configured (1-2 for prod)
- [ ] ILM policies for time-series data
- [ ] Templates for common indices

### Monitoring

- [ ] Kibana Monitoring enabled
- [ ] Alerts configured (heap, disk, cluster health)
- [ ] Logs aggregated (Filebeat â†’ Elasticsearch)
- [ ] Metrics exported (Prometheus/Grafana)
- [ ] Runbooks for common issues

### Ongoing Maintenance

- [ ] Monitor heap usage (< 75%)
- [ ] Check cluster health daily
- [ ] Review slow logs
- [ ] Test snapshot restoration
- [ ] Update Elasticsearch regularly
- [ ] Optimize queries based on slow logs
- [ ] Rebalance shards if needed

---

## Summary

**Elasticsearch is a complex distributed system built on:**
1. **Lucene**: The core search library (inverted index, segments)
2. **Distributed Architecture**: Shards, replicas, cluster coordination
3. **Memory Management**: Heap for structures, OS cache for segments
4. **Smart Caching**: Query cache, field data, request cache
5. **Horizontal Scalability**: Add nodes to scale

**Key Takeaways:**
- Understand shard sizing (20-40GB per shard)
- Keep heap â‰¤ 31GB, rest for OS cache
- Use filters (cacheable) over queries
- Bulk operations for indexing
- SSDs are essential for production
- Monitor heap, GC, cluster health
- Plan for growth with ILM

**Remember:**
> "Elasticsearch is easy to start with, hard to master. Understanding internals is the key to production success."

---

**Document created:** 2026-02-09  
**Companion to:** `elasticsearch-research.md`

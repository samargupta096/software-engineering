# Chapter 7: Finetuning

> *When prompting isn't enough â€” adapting models to your specific needs*

---

## ğŸ¯ Core Concepts

### Should You Finetune? (Decision Flowchart)

```mermaid
flowchart TD
    Start(["ğŸ¤” Considering Finetuning?"]) --> Q1{"Have you maxed out<br/>prompt engineering?"}
    Q1 -- No --> MaxPE["â¸ï¸ Go back and optimize<br/>your prompts first"]
    Q1 -- Yes --> Q2{"Do you have 500+<br/>high-quality examples?"}
    Q2 -- No --> Synth["ğŸ”„ Generate synthetic data<br/>or collect more examples"]
    Q2 -- Yes --> Q3{"Goal: style/format or<br/>domain adaptation?"}
    Q3 -- Yes --> FT["âœ… Finetune!<br/>Start with LoRA"]
    Q3 -- No --> Q4{"Cost reduction via<br/>smaller model?"}
    Q4 -- Yes --> Distill["âœ… Distillation:<br/>Large â†’ Small"]
    Q4 -- No --> Skip["âŒ Probably don't need<br/>finetuning"]

    style FT fill:#4CAF50,color:white
    style Distill fill:#4CAF50,color:white
    style MaxPE fill:#FF9800,color:white
    style Skip fill:#f44336,color:white
```

### When to Finetune vs. When NOT To

| âœ… Finetune When | âŒ Don't Finetune When |
| :--- | :--- |
| Need consistent style/format | Prompt engineering works well enough |
| Domain-specific language/jargon | Small dataset (< 200 examples) |
| Significant quality gap with prompting | Task changes frequently |
| Need to reduce prompt length/cost | Limited compute budget |
| Proprietary data can't be sent to APIs | You're just starting to explore |

### Types of Finetuning

```mermaid
flowchart TD
    FT["Finetuning Methods"] --> Full["ğŸ“¦ Full Finetuning<br/>Update ALL parameters"]
    FT --> PEFT["âš¡ Parameter-Efficient (PEFT)<br/>Update SMALL subset"]

    PEFT --> LoRA["LoRA<br/>Low-Rank Adaptation"]
    PEFT --> QLoRA["QLoRA<br/>LoRA + 4-bit quantization"]
    PEFT --> Prefix["Prefix Tuning<br/>Virtual token prefixes"]
    PEFT --> Adapter["Adapters<br/>Insert small trainable layers"]

    style Full fill:#ffcdd2,stroke:#c62828
    style PEFT fill:#c8e6c9,stroke:#388e3c
    style LoRA fill:#bbdefb,stroke:#1976d2
```

### PEFT Methods Compared

| Method | Trainable Params | VRAM Needed | Quality | Speed |
| :--- | :---: | :---: | :---: | :---: |
| **Full Finetuning** | 100% | Very High (80GB+) | â­â­â­â­â­ | Slow |
| **LoRA** | ~0.1-1% | Medium (16-24GB) | â­â­â­â­ | Fast |
| **QLoRA** | ~0.1-1% | Low (8-12GB) | â­â­â­â­ | Fast |
| **Prefix Tuning** | ~0.01% | Low | â­â­â­ | Very Fast |
| **Adapters** | ~1-5% | Medium | â­â­â­â­ | Fast |

### How LoRA Works

```mermaid
flowchart LR
    X["Input x"] --> W["W (dÃ—d)<br/>â„ï¸ FROZEN"]
    X --> A["A (dÃ—r)<br/>ğŸ”¥ Trainable"]
    A --> B["B (rÃ—d)<br/>ğŸ”¥ Trainable"]
    W --> Plus(("+"))
    B --> Plus
    Plus --> Y["Output y"]

    style W fill:#e3f2fd,stroke:#1976d2
    style A fill:#fff3e0,stroke:#ff9800
    style B fill:#fff3e0,stroke:#ff9800
```

```
Output = WÂ·x + AÂ·BÂ·x
Where rank r (4, 8, 16, 32) << d
Result: Train 0.1% of parameters, get ~95% of full finetune quality
```

### Finetuning Pipeline

```mermaid
flowchart LR
    Data["1ï¸âƒ£ Data Prep<br/>Format as<br/>instruction/output"] --> Base["2ï¸âƒ£ Choose Base<br/>Model<br/>(Llama, Mistral)"]
    Base --> Hyper["3ï¸âƒ£ Set Hyperparams<br/>LR, epochs,<br/>LoRA rank"]
    Hyper --> Train["4ï¸âƒ£ Train<br/>Monitor loss,<br/>avoid overfitting"]
    Train --> Eval["5ï¸âƒ£ Evaluate<br/>Compare vs.<br/>baseline"]
    Eval --> Deploy["6ï¸âƒ£ Merge & Deploy"]

    style Data fill:#e8eaf6,stroke:#3f51b5
    style Deploy fill:#c8e6c9,stroke:#388e3c
```

### Alignment Techniques

```mermaid
flowchart TD
    SFT["ğŸ“‹ SFT<br/>Supervised Fine-Tuning<br/>Train on instruction-response pairs"] --> RLHF["ğŸ† RLHF<br/>Reinforcement Learning<br/>from Human Feedback"]
    SFT --> DPO["âš¡ DPO<br/>Direct Preference Optimization<br/>Simpler than RLHF, no reward model"]

    RLHF --> RM["Train Reward Model"] --> PPO["Optimize with PPO"]
    DPO --> Pref["Learn directly from<br/>preference pairs"]

    style SFT fill:#e3f2fd,stroke:#1976d2
    style DPO fill:#c8e6c9,stroke:#388e3c
    style RLHF fill:#fff3e0,stroke:#ff9800
```

---

## ğŸ“ My Notes

<!-- Add your own notes, insights, and questions as you read -->



---

## â“ Questions to Reflect On

1. For your use case, would LoRA or full finetuning be more appropriate?
2. How much data would you need to finetune effectively?
3. How do you know if finetuning is working â€” what metrics do you track?
4. How do you handle model updates (new base model versions)?

---

## ğŸ”— Key Takeaways

1. 
2. 
3. 

---

## ğŸ› ï¸ Practice Ideas

- [ ] Finetune a small model (e.g., Llama 3.2 1B) with LoRA on a custom dataset
- [ ] Compare finetuned model vs. few-shot prompting on the same eval set
- [ ] Experiment with different LoRA ranks (4, 8, 16, 32) and measure impact
- [ ] Try DPO alignment on a preference dataset

---

<div align="center">

[â¬…ï¸ Previous Chapter](./chapter-06-rag-and-agents.md) | [ğŸ  Home](./README.md) | [Next Chapter â¡ï¸](./chapter-08-dataset-engineering.md)

</div>

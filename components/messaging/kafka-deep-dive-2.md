[ğŸ  Home](../README.md) | [â¬…ï¸ Components Overview](./00-components-overview.md)

# ğŸš€ Apache Kafka In-Depth Guide

> Master event streaming internals - from producers to KRaft consensus

---

## ğŸ“‹ Table of Contents

1. [What is Apache Kafka?](#-what-is-apache-kafka)
2. [Core Concepts](#-core-concepts)
3. [Architecture Overview](#-architecture-overview)
4. [KRaft Mode Deep Dive](#-kraft-mode-deep-dive)
5. [Producers Internals](#-producers-internals)
6. [Consumers Internals](#-consumers-internals)
7. [Partitioning Strategy](#-partitioning-strategy)
8. [Important Configurations](#-important-configurations)
9. [Idempotency & Transactions](#-idempotency--transactions)
10. [AWS MSK Deployment](#-aws-msk-deployment)
11. [Real-World Use Cases](#-real-world-use-cases)
12. [Visual Architecture (Mermaid Diagrams)](#-visual-architecture-mermaid-diagrams)
13. [Interview Questions & Answers](#-interview-questions--answers)

---

## ğŸ¯ What is Apache Kafka?

Apache Kafka is a **distributed event streaming platform** designed for high-throughput, fault-tolerant, real-time data pipelines. Unlike traditional message queues, Kafka persists messages and allows replay.

### Kafka vs Traditional Message Queues

| Feature | Kafka | RabbitMQ/ActiveMQ |
|---------|-------|-------------------|
| **Message Persistence** | Persists all messages | Typically deletes after consumption |
| **Replay Capability** | Yes, via offset seek | No |
| **Throughput** | Millions/sec | Thousands/sec |
| **Ordering** | Per partition | Per queue |
| **Consumer Model** | Pull-based | Push-based |
| **Use Case** | Event streaming, logs | Task queues, RPC |

### When to Use Kafka

âœ… **Use Kafka for:**
- Event sourcing & CQRS
- Real-time analytics pipelines
- Log aggregation
- Change Data Capture (CDC)
- Microservices event-driven communication

âŒ **Avoid Kafka for:**
- Simple request-reply patterns
- Low-volume messaging
- When message ordering across all messages is required

---

## ğŸ“š Core Concepts

### Key Terminology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      KAFKA ECOSYSTEM                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                       CLUSTER                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
â”‚  â”‚  â”‚ Broker 1â”‚  â”‚ Broker 2â”‚  â”‚ Broker 3â”‚                   â”‚   â”‚
â”‚  â”‚  â”‚(Ctrl+Brk)â”‚  â”‚ (Broker)â”‚  â”‚ (Broker)â”‚                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
â”‚  â”‚       â”‚            â”‚            â”‚                         â”‚   â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
â”‚  â”‚                    â”‚                                      â”‚   â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                â”‚   â”‚
â”‚  â”‚              â”‚   TOPIC   â”‚ (orders)                       â”‚   â”‚
â”‚  â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚   â”‚
â”‚  â”‚   â”‚          â”‚           â”‚          â”‚                     â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”´â”€â”      â”Œâ”€â”´â”€â”       â”Œâ”€â”´â”€â”      â”Œâ”€â”´â”€â”                   â”‚   â”‚
â”‚  â”‚ â”‚P0 â”‚      â”‚P1 â”‚       â”‚P2 â”‚      â”‚P3 â”‚  â† Partitions     â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜       â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜                   â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Producer â”€â”€â”€â”€â”€â”€â–º Topic â”€â”€â”€â”€â”€â”€â–º Consumer Group                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Term | Description |
|------|-------------|
| **Broker** | Kafka server that stores data and serves clients |
| **Topic** | Named category/feed to which records are published |
| **Partition** | Ordered, immutable sequence of records within a topic |
| **Offset** | Unique sequential ID for each record within a partition |
| **Producer** | Client that publishes records to topics |
| **Consumer** | Client that reads records from topics |
| **Consumer Group** | Set of consumers sharing the work of reading a topic |
| **Replica** | Copy of a partition for fault tolerance |
| **Leader** | Partition replica that handles all reads/writes |
| **ISR** | In-Sync Replicas - replicas caught up with leader |

### Message Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA RECORD (MESSAGE)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Headers (optional)                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
â”‚  â”‚  â”‚ correlation-id  â”‚ abc-123         â”‚                   â”‚   â”‚
â”‚  â”‚  â”‚ source          â”‚ payment-service â”‚                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Key (optional): "user-123"                              â”‚   â”‚
â”‚  â”‚  â€¢ Used for partitioning                                 â”‚   â”‚
â”‚  â”‚  â€¢ Same key â†’ Same partition                             â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Value: {"orderId": "ORD-456", "amount": 99.99}         â”‚   â”‚
â”‚  â”‚  â€¢ The actual message payload                            â”‚   â”‚
â”‚  â”‚  â€¢ Can be JSON, Avro, Protobuf, bytes                    â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Timestamp: 1706356800000                                â”‚   â”‚
â”‚  â”‚  â€¢ CreateTime or LogAppendTime                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Architecture Overview

### Cluster Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA CLUSTER ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    Controller Quorum                     â”‚   â”‚
â”‚   â”‚        (KRaft mode - manages metadata)                   â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚   â”‚  â”‚Controller â”‚ â”‚Controller â”‚ â”‚Controller â”‚              â”‚   â”‚
â”‚   â”‚  â”‚  (Active) â”‚ â”‚ (Follower)â”‚ â”‚ (Follower)â”‚              â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚   â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚ Metadata                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      â–¼                                   â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚   â”‚  â”‚  Broker 1   â”‚ â”‚  Broker 2   â”‚ â”‚  Broker 3   â”‚        â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚   â”‚
â”‚   â”‚  â”‚ Topic A     â”‚ â”‚ Topic A     â”‚ â”‚ Topic A     â”‚        â”‚   â”‚
â”‚   â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚   â”‚
â”‚   â”‚  â”‚ â”‚P0 Leaderâ”‚ â”‚ â”‚ â”‚P0 Followâ”‚ â”‚ â”‚ â”‚P1 Leaderâ”‚ â”‚        â”‚   â”‚
â”‚   â”‚  â”‚ â”‚P1 Followâ”‚ â”‚ â”‚ â”‚P1 Followâ”‚ â”‚ â”‚ â”‚P2 Leaderâ”‚ â”‚        â”‚   â”‚
â”‚   â”‚  â”‚ â”‚P2 Followâ”‚ â”‚ â”‚ â”‚P2 Followâ”‚ â”‚ â”‚ â”‚P0 Followâ”‚ â”‚        â”‚   â”‚
â”‚   â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚   â”‚                      DATA BROKERS                        â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   Clients: Producers & Consumers connect to Brokers             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Write Path

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WRITE PATH                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Producer                                                        â”‚
â”‚     â”‚                                                            â”‚
â”‚     â”‚ 1. Send record with key="user-123"                        â”‚
â”‚     â”‚    partition = hash("user-123") % num_partitions          â”‚
â”‚     â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚           Partition Leader              â”‚                    â”‚
â”‚  â”‚  1. Append to local log                 â”‚                    â”‚
â”‚  â”‚  2. Update LEO (Log End Offset)         â”‚                    â”‚
â”‚  â”‚  3. Wait for ISR acknowledgment         â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                   â”‚                                              â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚          â–¼                 â–¼                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚    â”‚ Follower â”‚      â”‚ Follower â”‚                               â”‚
â”‚    â”‚ Replica 1â”‚      â”‚ Replica 2â”‚                               â”‚
â”‚    â”‚ (fetch)  â”‚      â”‚ (fetch)  â”‚                               â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                  â”‚                                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                  â–¼                                               â”‚
â”‚         ACK sent to Leader                                       â”‚
â”‚                  â”‚                                               â”‚
â”‚                  â–¼                                               â”‚
â”‚         Leader ACKs Producer (based on acks setting)             â”‚
â”‚                                                                 â”‚
â”‚  acks=0  â†’ No wait (fire and forget)                            â”‚
â”‚  acks=1  â†’ Wait for leader only                                 â”‚
â”‚  acks=all â†’ Wait for all ISR replicas                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ KRaft Mode Deep Dive

### What is KRaft?

KRaft (Kafka Raft) is Kafka's **ZooKeeper-free** mode, using the Raft consensus algorithm for metadata management. Production-ready since Kafka 3.3+.

### ZooKeeper vs KRaft

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ZOOKEEPER MODE (Legacy)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   ZooKeeper    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         Kafka Cluster          â”‚  â”‚
â”‚  â”‚   Ensemble     â”‚         â”‚  â€¢ Broker metadata             â”‚  â”‚
â”‚  â”‚  (3-5 nodes)   â”‚         â”‚  â€¢ Topic configs               â”‚  â”‚
â”‚  â”‚                â”‚         â”‚  â€¢ ACLs                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  Problems:                                                       â”‚
â”‚  â€¢ Separate system to manage                                    â”‚
â”‚  â€¢ Different failure modes                                      â”‚
â”‚  â€¢ Scalability limited (~200K partitions)                       â”‚
â”‚  â€¢ Controller failover can take minutes                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KRAFT MODE (Modern)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Kafka Cluster                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚          Controller Quorum (Raft)               â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Active Controller (Leader)                   â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Follower Controllers                         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ __cluster_metadata topic                     â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚   Broker    â”‚ â”‚   Broker    â”‚ â”‚   Broker    â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Benefits:                                                       â”‚
â”‚  â€¢ Single system to manage                                      â”‚
â”‚  â€¢ Faster failover (seconds vs minutes)                         â”‚
â”‚  â€¢ Millions of partitions supported                             â”‚
â”‚  â€¢ Simpler operations                                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### KRaft Internal Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  KRAFT METADATA MANAGEMENT                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  __cluster_metadata Topic (Single Partition)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Offset 0: RegisterBrokerRecord(brokerId=1, ...)        â”‚   â”‚
â”‚  â”‚  Offset 1: TopicRecord(topicId=xyz, name="orders")      â”‚   â”‚
â”‚  â”‚  Offset 2: PartitionRecord(topicId=xyz, partitionId=0)  â”‚   â”‚
â”‚  â”‚  Offset 3: ConfigRecord(type=TOPIC, name="orders", ...) â”‚   â”‚
â”‚  â”‚  Offset 4: ProducerIdsRecord(brokerId=1, ...)           â”‚   â”‚
â”‚  â”‚  ...                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Controller Operations:                                          â”‚
â”‚  1. Append metadata changes to __cluster_metadata               â”‚
â”‚  2. Replicate via Raft to follower controllers                  â”‚
â”‚  3. Brokers fetch metadata updates                              â”‚
â”‚  4. Periodic snapshots to prevent unbounded growth              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Raft Consensus Protocol                     â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Leader Election:                                        â”‚   â”‚
â”‚  â”‚  â€¢ Followers timeout â†’ become candidates                 â”‚   â”‚
â”‚  â”‚  â€¢ Candidates request votes                              â”‚   â”‚
â”‚  â”‚  â€¢ Majority wins â†’ becomes leader                        â”‚   â”‚
â”‚  â”‚  â€¢ Leader sends heartbeats to maintain leadership        â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Log Replication:                                        â”‚   â”‚
â”‚  â”‚  â€¢ Leader appends entry with new epoch                   â”‚   â”‚
â”‚  â”‚  â€¢ Followers replicate (pull-based)                      â”‚   â”‚
â”‚  â”‚  â€¢ Entry committed when majority acknowledges            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### KRaft Configuration

```properties
# server.properties for KRaft mode

# Node roles: controller, broker, or both
process.roles=broker,controller

# Unique node ID
node.id=1

# Controller quorum voters (node.id@host:port)
controller.quorum.voters=1@localhost:9093,2@localhost:9094,3@localhost:9095

# Listeners
listeners=PLAINTEXT://:9092,CONTROLLER://:9093
controller.listener.names=CONTROLLER
inter.broker.listener.name=PLAINTEXT

# Log directories
log.dirs=/var/kafka-logs
metadata.log.dir=/var/kafka-metadata
```

---

## ğŸ“¤ Producers Internals

### Producer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCER INTERNALS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Application Thread                                              â”‚
â”‚       â”‚                                                          â”‚
â”‚       â”‚ producer.send(record)                                   â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Serializer                                           â”‚   â”‚
â”‚  â”‚     Key â†’ bytes, Value â†’ bytes                           â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  2. Partitioner                                          â”‚   â”‚
â”‚  â”‚     â€¢ If key != null: hash(key) % numPartitions          â”‚   â”‚
â”‚  â”‚     â€¢ If key == null: round-robin or sticky              â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  3. Record Accumulator (Buffer)                          â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚     â”‚ Batch 0 â”‚ â”‚ Batch 1 â”‚ â”‚ Batch 2 â”‚  â† Per partition â”‚   â”‚
â”‚  â”‚     â”‚ (P0)    â”‚ â”‚ (P1)    â”‚ â”‚ (P2)    â”‚    batching      â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â”‚ When batch.size reached OR            â”‚
â”‚                          â”‚ linger.ms elapsed                     â”‚
â”‚                          â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Sender Thread (Background)                              â”‚   â”‚
â”‚  â”‚  â€¢ Groups batches by broker                              â”‚   â”‚
â”‚  â”‚  â€¢ Sends ProduceRequest                                  â”‚   â”‚
â”‚  â”‚  â€¢ Handles retries (with backoff)                        â”‚   â”‚
â”‚  â”‚  â€¢ Manages in-flight requests                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚                    Kafka Broker                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Producer Configurations

| Config | Default | Description |
|--------|---------|-------------|
| `acks` | `all` (3.0+) | 0=no wait, 1=leader only, all=ISR |
| `retries` | `MAX_INT` | Number of retries on failure |
| `batch.size` | 16384 | Max batch size in bytes |
| `linger.ms` | 0 | Time to wait for more records |
| `buffer.memory` | 33554432 | Total buffer memory |
| `max.in.flight.requests.per.connection` | 5 | Concurrent requests per broker |
| `enable.idempotence` | `true` (3.0+) | Enable exactly-once per partition |

### Java Producer Example

```java
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

// Performance tuning
props.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768);      // 32KB batches
props.put(ProducerConfig.LINGER_MS_CONFIG, 20);          // Wait 20ms
props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4"); // Compress

// Reliability
props.put(ProducerConfig.ACKS_CONFIG, "all");
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);

try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
    ProducerRecord<String, String> record = new ProducerRecord<>(
        "orders",           // topic
        "user-123",         // key (determines partition)
        "{\"orderId\": 1}"  // value
    );
    
    // Async send with callback
    producer.send(record, (metadata, exception) -> {
        if (exception == null) {
            System.out.printf("Sent to partition %d, offset %d%n",
                metadata.partition(), metadata.offset());
        } else {
            exception.printStackTrace();
        }
    });
}
```

---

## ğŸ“¥ Consumers Internals

### Consumer Group Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CONSUMER GROUP ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Topic: orders (4 partitions)                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ P0  â”‚ â”‚ P1  â”‚ â”‚ P2  â”‚ â”‚ P3  â”‚                                â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜                                â”‚
â”‚     â”‚       â”‚       â”‚       â”‚                                    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Consumer Group: "order-processors"             â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚ Consumer 1  â”‚  â”‚ Consumer 2  â”‚  â”‚ Consumer 3  â”‚      â”‚   â”‚
â”‚  â”‚  â”‚ (P0, P1)    â”‚  â”‚ (P2)        â”‚  â”‚ (P3)        â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Group Coordinator (Broker)                              â”‚   â”‚
â”‚  â”‚  â€¢ Manages membership                                    â”‚   â”‚
â”‚  â”‚  â€¢ Triggers rebalance                                    â”‚   â”‚
â”‚  â”‚  â€¢ Stores committed offsets                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Rules:                                                          â”‚
â”‚  â€¢ Each partition â†’ exactly one consumer per group              â”‚
â”‚  â€¢ More consumers than partitions â†’ some idle                   â”‚
â”‚  â€¢ Consumer leaves â†’ partitions redistributed                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consumer Rebalancing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REBALANCE TRIGGERS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Rebalance happens when:                                         â”‚
â”‚  â€¢ Consumer joins the group                                     â”‚
â”‚  â€¢ Consumer leaves (graceful shutdown)                          â”‚
â”‚  â€¢ Consumer crashes (heartbeat timeout)                         â”‚
â”‚  â€¢ New partitions added to topic                                â”‚
â”‚  â€¢ Subscription changes                                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              EAGER REBALANCE (Legacy)                    â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  1. All consumers stop processing                        â”‚   â”‚
â”‚  â”‚  2. All consumers revoke ALL partitions                  â”‚   â”‚
â”‚  â”‚  3. Group coordinator reassigns                          â”‚   â”‚
â”‚  â”‚  4. All consumers resume with new assignments            â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Problem: Complete processing pause                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           COOPERATIVE REBALANCE (Recommended)            â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  1. Consumers continue processing                        â”‚   â”‚
â”‚  â”‚  2. Only affected partitions are revoked                 â”‚   â”‚
â”‚  â”‚  3. Incremental reassignment                             â”‚   â”‚
â”‚  â”‚  4. Minimal disruption                                   â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Enable: partition.assignment.strategy=                  â”‚   â”‚
â”‚  â”‚          CooperativeStickyAssignor                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Offset Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OFFSET MANAGEMENT                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Partition Log:                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ 0  â”‚ 1  â”‚ 2  â”‚ 3  â”‚ 4  â”‚ 5  â”‚ 6  â”‚ 7  â”‚ 8  â”‚ 9  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜            â”‚
â”‚                          â–²              â–²                        â”‚
â”‚                          â”‚              â”‚                        â”‚
â”‚              Committed Offset (5)   Current Position (8)         â”‚
â”‚              (stored in            (in-memory, where             â”‚
â”‚               __consumer_offsets)   consumer will read next)     â”‚
â”‚                                                                 â”‚
â”‚  Commit Strategies:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AUTO COMMIT (enable.auto.commit=true)                   â”‚   â”‚
â”‚  â”‚  â€¢ Commits every auto.commit.interval.ms (5000ms)        â”‚   â”‚
â”‚  â”‚  â€¢ Risk: Message loss or duplicates on crash             â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  MANUAL SYNC (commitSync())                              â”‚   â”‚
â”‚  â”‚  â€¢ Blocks until commit confirmed                         â”‚   â”‚
â”‚  â”‚  â€¢ Slowest but safest                                    â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  MANUAL ASYNC (commitAsync())                            â”‚   â”‚
â”‚  â”‚  â€¢ Non-blocking, uses callback                           â”‚   â”‚
â”‚  â”‚  â€¢ Best for throughput                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Java Consumer Example

```java
Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ConsumerConfig.GROUP_ID_CONFIG, "order-processors");
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

// Offset management
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

// Rebalance strategy
props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,
          CooperativeStickyAssignor.class.getName());

try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
    consumer.subscribe(List.of("orders"));
    
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        
        for (ConsumerRecord<String, String> record : records) {
            System.out.printf("Partition: %d, Offset: %d, Key: %s, Value: %s%n",
                record.partition(), record.offset(), record.key(), record.value());
            
            // Process record...
        }
        
        // Manual commit after processing
        consumer.commitSync();
    }
}
```

---

## ğŸ“Š Partitioning Strategy

### How to Decide Number of Partitions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PARTITION COUNT DECISION FRAMEWORK                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  FORMULA:                                                        â”‚
â”‚                                                                 â”‚
â”‚  partitions = max(Tp/p, Tc/c)                                   â”‚
â”‚                                                                 â”‚
â”‚  Where:                                                          â”‚
â”‚  â€¢ Tp = Target throughput (MB/s)                                â”‚
â”‚  â€¢ p  = Throughput per producer partition (~10 MB/s)            â”‚
â”‚  â€¢ Tc = Target throughput (MB/s)                                â”‚
â”‚  â€¢ c  = Throughput per consumer partition (~5-10 MB/s)          â”‚
â”‚                                                                 â”‚
â”‚  Example:                                                        â”‚
â”‚  â€¢ Need 100 MB/s throughput                                     â”‚
â”‚  â€¢ Producer: 100 / 10 = 10 partitions                           â”‚
â”‚  â€¢ Consumer: 100 / 5 = 20 partitions                            â”‚
â”‚  â€¢ Result: max(10, 20) = 20 partitions                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSIDERATIONS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âœ… MORE PARTITIONS:                                             â”‚
â”‚  â€¢ Higher parallelism                                           â”‚
â”‚  â€¢ More consumers can process simultaneously                    â”‚
â”‚  â€¢ Better scalability                                           â”‚
â”‚                                                                 â”‚
â”‚  âŒ TOO MANY PARTITIONS:                                         â”‚
â”‚  â€¢ More file handles on brokers                                 â”‚
â”‚  â€¢ Longer leader election time                                  â”‚
â”‚  â€¢ More memory for consumers                                    â”‚
â”‚  â€¢ Increased end-to-end latency                                 â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“‹ GUIDELINES:                                                  â”‚
â”‚  â€¢ Start with 6-12 partitions for new topics                    â”‚
â”‚  â€¢ Plan for 12-18 month growth                                  â”‚
â”‚  â€¢ Choose divisible numbers (12, 24, 48)                        â”‚
â”‚  â€¢ Max ~4000 partitions per broker                              â”‚
â”‚  â€¢ Cannot reduce partitions (only increase)                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Partitioning Strategies

| Strategy | Use Case | Ordering Guarantee |
|----------|----------|-------------------|
| **Key-based** | Events for same entity together | Per key |
| **Round-robin** | Max throughput, no ordering needed | None |
| **Custom** | Business logic (region, priority) | Depends |

```java
// Custom partitioner example
public class RegionPartitioner implements Partitioner {
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        String region = extractRegion(key);
        int numPartitions = cluster.partitionsForTopic(topic).size();
        
        return switch(region) {
            case "US" -> 0;
            case "EU" -> numPartitions / 3;
            case "APAC" -> 2 * numPartitions / 3;
            default -> Math.abs(key.hashCode()) % numPartitions;
        };
    }
}
```

---

## ğŸ”’ Idempotency & Transactions

### Delivery Semantics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DELIVERY SEMANTICS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  AT-MOST-ONCE:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Producer â”€â”€sendâ”€â”€â–º Broker              â”‚                    â”‚
â”‚  â”‚     â”‚         âœ— (no retry on failure)   â”‚                    â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â–º Message may be LOST        â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                 â”‚
â”‚  AT-LEAST-ONCE (Default):                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Producer â”€â”€sendâ”€â”€â–º Broker              â”‚                    â”‚
â”‚  â”‚     â”‚         timeout                    â”‚                    â”‚
â”‚  â”‚     â””â”€â”€retryâ”€â”€â–º Broker                  â”‚                    â”‚
â”‚  â”‚         âœ“âœ“ (Message may be DUPLICATED)  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                 â”‚
â”‚  EXACTLY-ONCE (Idempotent + Transactional):                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Producer â”€â”€send(PID, SeqNum)â”€â”€â–º Broker â”‚                    â”‚
â”‚  â”‚     â”‚         timeout                    â”‚                    â”‚
â”‚  â”‚     â””â”€â”€retry(PID, SeqNum)â”€â”€â–º Broker     â”‚                    â”‚
â”‚  â”‚         Broker detects duplicate        â”‚                    â”‚
â”‚  â”‚         âœ“ (One and only one)            â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Idempotent Producer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  IDEMPOTENT PRODUCER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  How it works:                                                   â”‚
â”‚                                                                 â”‚
â”‚  1. Producer gets PID (Producer ID) from broker                 â”‚
â”‚  2. Each message gets sequence number per partition             â”‚
â”‚  3. Broker tracks: {PID, PartitionId} â†’ LastSeqNum              â”‚
â”‚  4. On retry, broker detects duplicate and rejects              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Producer (PID=42)                                       â”‚   â”‚
â”‚  â”‚     â”‚                                                    â”‚   â”‚
â”‚  â”‚     â”‚ Send: {PID=42, Partition=0, SeqNum=5, msg="A"}     â”‚   â”‚
â”‚  â”‚     â”‚                                                    â”‚   â”‚
â”‚  â”‚     â–¼                                                    â”‚   â”‚
â”‚  â”‚  Broker                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚ Seq Tracker: {(42,0): 4}                 â”‚           â”‚   â”‚
â”‚  â”‚  â”‚                                          â”‚           â”‚   â”‚
â”‚  â”‚  â”‚ Incoming SeqNum=5, Expected=5 âœ“ Accept   â”‚           â”‚   â”‚
â”‚  â”‚  â”‚ Update: {(42,0): 5}                      â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  If network timeout causes retry with same SeqNum:       â”‚   â”‚
â”‚  â”‚  Incoming SeqNum=5, Last=5 â†’ Duplicate! Reject           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Configuration:                                                  â”‚
â”‚  enable.idempotence=true (default in Kafka 3.0+)                â”‚
â”‚  max.in.flight.requests.per.connection â‰¤ 5                      â”‚
â”‚  acks=all                                                       â”‚
â”‚  retries > 0                                                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Transactional Producer

```java
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "order-processor-1");
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);

KafkaProducer<String, String> producer = new KafkaProducer<>(props);

// Initialize transactions (call once)
producer.initTransactions();

try {
    // Start transaction
    producer.beginTransaction();
    
    // Send multiple messages atomically
    producer.send(new ProducerRecord<>("orders", "key1", "order1"));
    producer.send(new ProducerRecord<>("payments", "key1", "payment1"));
    producer.send(new ProducerRecord<>("inventory", "key1", "reserve1"));
    
    // Commit transaction (all or nothing)
    producer.commitTransaction();
    
} catch (Exception e) {
    // Abort transaction on failure
    producer.abortTransaction();
    throw e;
}
```

### Read-Process-Write Pattern (Exactly-Once)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONSUME-TRANSFORM-PRODUCE PATTERN                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Source   â”‚ â”€â”€â–ºâ”‚    Application    â”‚ â”€â”€â–ºâ”‚  Target   â”‚       â”‚
â”‚  â”‚  Topic    â”‚    â”‚                   â”‚    â”‚  Topic    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  consumer.poll()  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                   â”‚  process()        â”‚                         â”‚
â”‚                   â”‚  producer.send()  â”‚                         â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                 â”‚
â”‚  WITHOUT TRANSACTIONS:                                           â”‚
â”‚  â€¢ Process message â†’ produce output â†’ commit offset             â”‚
â”‚  â€¢ Crash after produce but before commit = DUPLICATE            â”‚
â”‚  â€¢ Commit before produce = MESSAGE LOSS                         â”‚
â”‚                                                                 â”‚
â”‚  WITH TRANSACTIONS:                                              â”‚
â”‚  producer.beginTransaction();                                    â”‚
â”‚  producer.send(outputRecord);                                   â”‚
â”‚  producer.sendOffsetsToTransaction(consumedOffsets, groupId);   â”‚
â”‚  producer.commitTransaction();                                   â”‚
â”‚                                                                 â”‚
â”‚  â†’ Offset commit and output are ATOMIC                          â”‚
â”‚  â†’ Consumer uses: isolation.level=read_committed                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Important Configurations

### Broker Configurations

| Config | Default | Recommendation | Purpose |
|--------|---------|----------------|---------|
| `num.partitions` | 1 | 6-12 | Default partitions for new topics |
| `default.replication.factor` | 1 | 3 | Replicas for fault tolerance |
| `min.insync.replicas` | 1 | 2 | Min replicas for acks=all |
| `log.retention.hours` | 168 (7d) | Based on use case | How long to keep data |
| `log.segment.bytes` | 1GB | 1GB | Segment file size |
| `message.max.bytes` | 1MB | 1-10MB | Max message size |

### Producer Configurations

| Config | Default | High Throughput | High Reliability |
|--------|---------|-----------------|------------------|
| `acks` | all | 1 | all |
| `batch.size` | 16KB | 64KB-128KB | 16KB |
| `linger.ms` | 0 | 20-100 | 5 |
| `compression.type` | none | lz4/snappy | lz4 |
| `buffer.memory` | 32MB | 64MB+ | 32MB |
| `enable.idempotence` | true | true | true |

### Consumer Configurations

| Config | Default | Recommendation | Purpose |
|--------|---------|----------------|---------|
| `fetch.min.bytes` | 1 | 1-1024 | Min data before returning |
| `fetch.max.wait.ms` | 500 | 100-500 | Max wait time |
| `max.poll.records` | 500 | 100-1000 | Records per poll |
| `max.poll.interval.ms` | 300000 | Processing time + buffer | Max time between polls |
| `session.timeout.ms` | 45000 | 10000-30000 | Heartbeat timeout |
| `auto.offset.reset` | latest | earliest/latest | Behavior when no offset |

---

## â˜ï¸ AWS MSK Deployment

### MSK Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AWS MSK CLUSTER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         VPC                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚     AZ-a    â”‚ â”‚     AZ-b    â”‚ â”‚     AZ-c    â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  â”‚Broker1â”‚  â”‚ â”‚  â”‚Broker2â”‚  â”‚ â”‚  â”‚Broker3â”‚  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚             â”‚ â”‚             â”‚ â”‚             â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  EBS  â”‚  â”‚ â”‚  â”‚  EBS  â”‚  â”‚ â”‚  â”‚  EBS  â”‚  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Security: IAM Auth / SASL/SCRAM / mTLS                  â”‚   â”‚
â”‚  â”‚  Encryption: At-rest (KMS) + In-transit (TLS)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Monitoring: CloudWatch + Prometheus (Open Monitoring)          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MSK Best Practices

| Category | Recommendation |
|----------|----------------|
| **Sizing** | CPU utilization < 60%, plan for 3 AZs |
| **Storage** | Use provisioned throughput for predictable performance |
| **Security** | Enable IAM auth, TLS, encrypt at rest with KMS |
| **Monitoring** | Enable Enhanced monitoring, set CloudWatch alarms |
| **Networking** | Private subnets, VPC endpoints for clients |

### MSK Configuration Example

```java
// Spring Boot application.yml for AWS MSK
spring:
  kafka:
    bootstrap-servers: ${MSK_BOOTSTRAP_SERVERS}
    security:
      protocol: SASL_SSL
    properties:
      sasl.mechanism: AWS_MSK_IAM
      sasl.jaas.config: software.amazon.msk.auth.iam.IAMLoginModule required;
      sasl.client.callback.handler.class: software.amazon.msk.auth.iam.IAMClientCallbackHandler
    ssl:
      trust-store-location: classpath:kafka.client.truststore.jks
```

---

## ğŸŒ Real-World Use Cases

### Use Case 1: Event Sourcing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EVENT SOURCING                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Commands â”€â”€â–º Event Store (Kafka) â”€â”€â–º Projections               â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Topic: account-events                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ Key: acc-123 â”‚ AccountCreated {balance: 0}       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Key: acc-123 â”‚ MoneyDeposited {amount: 100}      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Key: acc-123 â”‚ MoneyWithdrawn {amount: 50}       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Key: acc-123 â”‚ MoneyDeposited {amount: 200}      â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Current Balance = replay(events) = 250                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Benefits:                                                       â”‚
â”‚  â€¢ Complete audit trail                                         â”‚
â”‚  â€¢ Rebuild state at any point in time                           â”‚
â”‚  â€¢ Easy debugging (replay events)                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Use Case 2: Log Aggregation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LOG AGGREGATION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚Service Aâ”‚  â”‚Service Bâ”‚  â”‚Service Câ”‚                         â”‚
â”‚  â”‚  logs   â”‚  â”‚  logs   â”‚  â”‚  logs   â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚
â”‚       â”‚            â”‚            â”‚                               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                    â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚             Kafka Topic: logs                            â”‚   â”‚
â”‚  â”‚  â€¢ Partitioned by service                                â”‚   â”‚
â”‚  â”‚  â€¢ Retention: 7 days                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                            â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚       â–¼            â–¼            â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚Elastic- â”‚  â”‚  S3     â”‚  â”‚Real-timeâ”‚                         â”‚
â”‚  â”‚search   â”‚  â”‚ Archive â”‚  â”‚Alerting â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Use Case 3: Microservices Communication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EVENT-DRIVEN MICROSERVICES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Order    â”‚  â”€â”€â”€â”€â”€â–º â”‚   Kafka    â”‚  â”€â”€â”€â”€â”€â–º â”‚  Payment   â”‚  â”‚
â”‚  â”‚  Service   â”‚         â”‚            â”‚         â”‚  Service   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚ â”‚orders  â”‚ â”‚                          â”‚
â”‚                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                         â”‚            â”‚  â”€â”€â”€â”€â”€â–º â”‚ Inventory  â”‚  â”‚
â”‚                         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚  Service   â”‚  â”‚
â”‚                         â”‚ â”‚paymentsâ”‚ â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                          â”‚
â”‚                         â”‚            â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”€â”€â”€â”€â”€â–º â”‚Notificationâ”‚  â”‚
â”‚                         â”‚ â”‚notif.  â”‚ â”‚         â”‚  Service   â”‚  â”‚
â”‚                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                 â”‚
â”‚  Pattern: Saga / Choreography                                   â”‚
â”‚  â€¢ OrderCreated â†’ PaymentService validates                      â”‚
â”‚  â€¢ PaymentProcessed â†’ InventoryService reserves                 â”‚
â”‚  â€¢ InventoryReserved â†’ NotificationService sends email          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Quick Reference

### CLI Commands

```bash
# Create topic
kafka-topics.sh --create --topic orders \
  --partitions 12 --replication-factor 3 \
  --bootstrap-server localhost:9092

# List topics
kafka-topics.sh --list --bootstrap-server localhost:9092

# Describe topic
kafka-topics.sh --describe --topic orders \
  --bootstrap-server localhost:9092

# Produce messages
kafka-console-producer.sh --topic orders \
  --bootstrap-server localhost:9092

# Consume messages
kafka-console-consumer.sh --topic orders \
  --from-beginning --bootstrap-server localhost:9092

# Consumer groups
kafka-consumer-groups.sh --list --bootstrap-server localhost:9092
kafka-consumer-groups.sh --describe --group my-group \
  --bootstrap-server localhost:9092
```

### Spring Kafka Annotations

```java
// Producer
@Autowired
private KafkaTemplate<String, String> kafkaTemplate;

kafkaTemplate.send("topic", "key", "value");

// Consumer
@KafkaListener(topics = "orders", groupId = "order-processors")
public void listen(ConsumerRecord<String, String> record) {
    // Process record
}

// With error handling
@KafkaListener(topics = "orders")
@RetryableTopic(
    attempts = "3",
    backoff = @Backoff(delay = 1000, multiplier = 2)
)
public void listen(String message) {
    // Process with retry
}

@DltHandler
public void handleDlt(String message) {
    // Handle dead letter
}
```

---

## ğŸ“Š Visual Architecture (Mermaid Diagrams)

### Complete Kafka Flow

```mermaid
flowchart TB
    subgraph Producers["ğŸ“¤ Producers"]
        P1[Service A]
        P2[Service B]
        P3[Mobile App]
    end

    subgraph Kafka["Apache Kafka Cluster"]
        subgraph Controllers["Controller Quorum (KRaft)"]
            C1[Controller 1<br>Leader]
            C2[Controller 2]
            C3[Controller 3]
        end
        
        subgraph Brokers["Data Brokers"]
            B1[Broker 1]
            B2[Broker 2]
            B3[Broker 3]
        end
        
        subgraph Topics["Topics"]
            T1[orders<br>P0, P1, P2]
            T2[payments<br>P0, P1]
        end
    end

    subgraph Consumers["ğŸ“¥ Consumer Groups"]
        CG1[Order Processors<br>3 consumers]
        CG2[Analytics Pipeline<br>2 consumers]
    end

    P1 --> T1
    P2 --> T1
    P3 --> T2
    T1 --> CG1
    T1 --> CG2
    T2 --> CG1
    
    C1 -.->|metadata| B1
    C1 -.->|metadata| B2
    C1 -.->|metadata| B3
```

### Producer Internal Flow

```mermaid
flowchart LR
    subgraph Application
        A[Your Code]
    end
    
    subgraph Producer["Kafka Producer"]
        S[Serializer]
        P[Partitioner]
        RA[Record Accumulator]
        subgraph Batches["Batches per Partition"]
            B0[Batch P0]
            B1[Batch P1]
            B2[Batch P2]
        end
        SENDER[Sender Thread]
    end
    
    subgraph Brokers["Kafka Brokers"]
        BR[Broker]
    end

    A -->|send| S
    S --> P
    P --> RA
    RA --> B0
    RA --> B1
    RA --> B2
    B0 -->|batch.size OR linger.ms| SENDER
    B1 --> SENDER
    B2 --> SENDER
    SENDER -->|ProduceRequest| BR
    BR -->|ACK| SENDER
```

### Consumer Rebalancing Flow

```mermaid
sequenceDiagram
    participant C1 as Consumer 1
    participant C2 as Consumer 2
    participant GC as Group Coordinator
    participant C3 as New Consumer 3

    Note over C1,C2: Initial State: C1(P0,P1), C2(P2,P3)
    
    C3->>GC: JoinGroup Request
    GC->>C1: Rebalance Triggered
    GC->>C2: Rebalance Triggered
    
    Note over C1,C3: Cooperative Rebalance
    C1->>GC: JoinGroup (current: P0,P1)
    C2->>GC: JoinGroup (current: P2,P3)
    C3->>GC: JoinGroup (new member)
    
    GC->>GC: Calculate new assignment
    
    GC->>C1: SyncGroup (P0)
    GC->>C2: SyncGroup (P2,P3)
    GC->>C3: SyncGroup (P1)
    
    C1->>C1: Revoke P1 only
    Note over C1,C3: Final: C1(P0), C2(P2,P3), C3(P1)
```

### Transaction Flow

```mermaid
sequenceDiagram
    participant P as Producer
    participant TC as Transaction Coordinator
    participant B1 as Broker (orders)
    participant B2 as Broker (payments)

    P->>TC: InitTransactions(txn.id)
    TC-->>P: PID assigned
    
    P->>TC: beginTransaction()
    
    P->>B1: send(orders, msg1)
    B1-->>P: ACK
    
    P->>B2: send(payments, msg2)
    B2-->>P: ACK
    
    P->>TC: commitTransaction()
    TC->>B1: WriteTxnMarker(COMMIT)
    TC->>B2: WriteTxnMarker(COMMIT)
    TC-->>P: Transaction Committed
    
    Note over P,B2: Both messages visible atomically
```

### Partition Leader Election

```mermaid
flowchart TB
    subgraph Topic["Topic: orders, Partition 0"]
        L[Broker 1<br>LEADER]
        F1[Broker 2<br>FOLLOWER]
        F2[Broker 3<br>FOLLOWER]
    end
    
    subgraph ISR["In-Sync Replicas"]
        ISR1[Broker 1 âœ“]
        ISR2[Broker 2 âœ“]
        ISR3[Broker 3 âœ“]
    end

    L -->|replicate| F1
    L -->|replicate| F2
    
    Client[Producer/Consumer] -->|read/write| L
    
    style L fill:#22c55e
    style F1 fill:#3b82f6
    style F2 fill:#3b82f6
```

---

## ğŸ¯ Interview Questions & Answers

### Beginner Level

**Q1: What is Apache Kafka and how does it differ from traditional message queues?**

**Answer:** Apache Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant data pipelines. Key differences:

| Aspect | Kafka | Traditional MQ (RabbitMQ) |
|--------|-------|---------------------------|
| Storage | Persists messages on disk | Deletes after consumption |
| Replay | Consumers can replay by resetting offset | Not possible |
| Consumption | Pull-based | Push-based |
| Throughput | Millions messages/sec | Thousands/sec |
| Ordering | Per partition | Per queue |

---

**Q2: Explain Topic, Partition, and Offset.**

**Answer:**
- **Topic**: Logical category/feed for messages (like a database table)
- **Partition**: Ordered, immutable log. Each topic has 1+ partitions for parallelism
- **Offset**: Unique sequential ID for each message within a partition

```
Topic "orders" (3 partitions):
P0: [0][1][2][3][4] â†’ offset 0-4
P1: [0][1][2]       â†’ offset 0-2  
P2: [0][1][2][3]    â†’ offset 0-3
```

---

**Q3: What is a Consumer Group?**

**Answer:** A Consumer Group is a set of consumers that collaborate to consume messages from a topic:
- Each partition is consumed by **exactly one consumer** in the group
- Multiple consumer groups can consume the same topic independently
- Enables horizontal scaling of consumption

**Rule:** If consumers > partitions, some consumers will be idle.

---

### Intermediate Level

**Q4: Explain the difference between `acks=0`, `acks=1`, and `acks=all`.**

**Answer:**

| Setting | Behavior | Durability | Performance |
|---------|----------|------------|-------------|
| `acks=0` | Fire and forget, no wait | Message may be lost | Fastest |
| `acks=1` | Wait for leader ACK | May lose if leader fails before replication | Medium |
| `acks=all` | Wait for ALL ISR replicas | Highest durability | Slowest |

**Production recommendation:** Use `acks=all` with `min.insync.replicas=2` for critical data.

---

**Q5: What is an Idempotent Producer and how does it work?**

**Answer:** An idempotent producer ensures exactly-once delivery per partition by:

1. **Producer ID (PID)**: Assigned by broker at startup
2. **Sequence Number**: Each message gets incrementing sequence per partition
3. **Deduplication**: Broker tracks `{PID, Partition} â†’ LastSeqNum`

```
Producer sends: {PID=42, Partition=0, SeqNum=5}
Broker tracks: Last SeqNum for (42,0) = 4
If SeqNum=5 â†’ Accept (expected)
If SeqNum=5 again (retry) â†’ Duplicate! Reject

Config: enable.idempotence=true (default in Kafka 3.0+)
```

---

**Q6: What triggers a Consumer Group rebalance?**

**Answer:** Rebalances occur when:
- âœ… Consumer joins the group
- âœ… Consumer leaves gracefully
- âœ… Consumer crashes (heartbeat timeout)
- âœ… New partitions added to subscribed topic
- âœ… Consumer subscription changes

**Minimizing rebalance impact:**
```properties
partition.assignment.strategy=CooperativeStickyAssignor
session.timeout.ms=30000
heartbeat.interval.ms=10000
```

---

**Q7: Explain the difference between Eager and Cooperative rebalancing.**

**Answer:**

| Aspect | Eager (Legacy) | Cooperative (Recommended) |
|--------|----------------|---------------------------|
| Revocation | ALL partitions from ALL consumers | Only affected partitions |
| Processing | Complete stop-the-world | Minimal disruption |
| Iterations | Single round | May need multiple rounds |
| Config | RangeAssignor, RoundRobinAssignor | CooperativeStickyAssignor |

---

### Advanced Level

**Q8: What is KRaft mode and why was ZooKeeper removed?**

**Answer:** KRaft (Kafka Raft) is Kafka's ZooKeeper-free mode (production-ready since 3.3+):

**Problems with ZooKeeper:**
- Separate distributed system to manage
- Controller failover took minutes
- Limited to ~200K partitions
- Different failure modes

**KRaft Benefits:**
- Single system architecture
- Failover in seconds
- Millions of partitions supported
- Metadata stored in `__cluster_metadata` topic
- Uses Raft consensus for leader election

---

**Q9: Explain exactly-once semantics with Kafka transactions.**

**Answer:** Exactly-once requires three components:

1. **Idempotent Producer**: Prevents duplicates per partition
2. **Transactional Producer**: Atomic writes across multiple topics
3. **Consumer `isolation.level=read_committed`**: Only reads committed messages

```java
producer.initTransactions();
producer.beginTransaction();
producer.send(new ProducerRecord<>("topic1", msg1));
producer.send(new ProducerRecord<>("topic2", msg2));
producer.sendOffsetsToTransaction(offsets, groupId);
producer.commitTransaction(); // Atomic!
```

**Use case:** Read-process-write pattern (consume â†’ transform â†’ produce)

---

**Q10: How do you decide the number of partitions for a topic?**

**Answer:** Use the formula:

```
partitions = max(Tp/p, Tc/c)

Where:
Tp = Target producer throughput (MB/s)
p = Throughput per producer (~10 MB/s)
Tc = Target consumer throughput (MB/s)
c = Throughput per consumer (~5-10 MB/s)
```

**Example:** Need 100 MB/s â†’ `max(100/10, 100/5) = 20 partitions`

**Guidelines:**
- Start with 6-12 for new topics
- Plan for 12-18 month growth
- Max ~4000 partitions per broker
- Cannot reduce partitions (only increase)

---

**Q11: What is Log Compaction and when would you use it?**

**Answer:** Log compaction keeps only the **latest value per key**, deleting older messages with the same key.

```
Before compaction:
[k1:v1][k2:v1][k1:v2][k3:v1][k1:v3]

After compaction:
[k2:v1][k3:v1][k1:v3]  â† Only latest per key
```

**Use cases:**
- Changelog for database (CDC)
- State storage (key-value store)
- Event sourcing snapshots

**Config:** `cleanup.policy=compact`

---

**Q12: How does Kafka achieve high throughput?**

**Answer:** Multiple optimizations:

| Technique | Explanation |
|-----------|-------------|
| **Sequential I/O** | Append-only log, no random seeks |
| **Zero-copy** | sendfile() syscall bypasses user space |
| **Batching** | Groups messages for efficient I/O |
| **Compression** | LZ4/Snappy reduces network/disk |
| **Page cache** | Linux memory cache for hot data |
| **Partitioning** | Parallelism across brokers |

---

**Q13: Explain the ISR (In-Sync Replicas) concept.**

**Answer:** ISR is the set of replicas that are "caught up" with the leader:

- **Leader**: Handles all reads/writes
- **Followers**: Fetch and replicate from leader
- **ISR**: Replicas within `replica.lag.time.max.ms` (default 30s)

**Failover:** Only ISR members can become new leader

**Config:**
```properties
min.insync.replicas=2  # At least 2 replicas in ISR
acks=all               # Wait for all ISR to acknowledge
```

If ISR < `min.insync.replicas`, producer receives `NotEnoughReplicasException`.

---

**Q14: What is the difference between `at-most-once`, `at-least-once`, and `exactly-once`?**

**Answer:**

| Semantics | Behavior | Config |
|-----------|----------|--------|
| **At-most-once** | Message may be lost, no duplicates | `acks=0` or commit before process |
| **At-least-once** | Message delivered, may have duplicates | `acks=all` + retries (default) |
| **Exactly-once** | Message delivered once, no loss/duplicates | Idempotent + Transactional producer |

---

**Q15: How would you handle poison pill messages (messages that always fail)?**

**Answer:** Implement Dead Letter Queue (DLQ) pattern:

```java
@RetryableTopic(
    attempts = "4",
    backoff = @Backoff(delay = 1000, multiplier = 2),
    dltTopicSuffix = "-dlt"
)
@KafkaListener(topics = "orders")
public void process(Order order) {
    // May throw exception
}

@DltHandler
public void handleDlt(Order order) {
    // Log, store in DB, alert ops team
}
```

**Flow:** `orders` â†’ retry 3x â†’ `orders-dlt` (dead letter topic)

---

## ğŸ“– Further Reading

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Confluent Developer](https://developer.confluent.io/)
- [AWS MSK Documentation](https://docs.aws.amazon.com/msk/)
- [KRaft Architecture (KIP-500)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-500)
- [Designing Event-Driven Systems (O'Reilly)](https://www.confluent.io/designing-event-driven-systems/)

---

*Last updated: January 2026*

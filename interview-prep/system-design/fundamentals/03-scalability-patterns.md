[ğŸ  Home](../../../README.md) | [â¬…ï¸ CAP Theorem](./02-cap-theorem.md) | [â¡ï¸ PostgreSQL Guide](./04-postgresql-guide.md)

# ğŸ“ˆ Scalability Patterns

> How to design systems that handle millions of users

---

## ğŸ¯ Quick Reference: When to Use What

| Pattern | Problem It Solves | Example |
|---------|-------------------|---------|
| **Replication** | Read bottleneck | MySQL read replicas |
| **Sharding** | Data too big for one DB | User ID-based sharding |
| **Caching** | Repeated expensive queries | Redis for sessions |
| **Async Processing** | Slow operations blocking users | Email via queue |
| **CDN** | Slow static content delivery | CloudFront for images |
| **Load Balancing** | Single server overload | Nginx round-robin |

---

## ğŸ“Š The Scaling Journey

```mermaid
flowchart TB
    subgraph Stage1["Stage 1: 1-1K Users"]
        S1_APP[Single Server]
        S1_DB[(Single DB)]
        S1_APP --> S1_DB
    end
    
    subgraph Stage2["Stage 2: 1K-100K Users"]
        S2_LB[Load Balancer]
        S2_APP1[App Server 1]
        S2_APP2[App Server 2]
        S2_CACHE[(Redis Cache)]
        S2_DB[(Master DB)]
        S2_REP[(Replica DB)]
        
        S2_LB --> S2_APP1 & S2_APP2
        S2_APP1 & S2_APP2 --> S2_CACHE --> S2_DB
        S2_DB --> S2_REP
    end
    
    subgraph Stage3["Stage 3: 100K-1M Users"]
        S3_CDN[CDN]
        S3_LB[Load Balancer]
        S3_APPS[Multiple App Servers]
        S3_CACHE[(Redis Cluster)]
        S3_SHARDS[(Sharded DBs)]
        S3_QUEUE[Message Queue]
        S3_WORKERS[Background Workers]
        
        S3_CDN --> S3_LB --> S3_APPS
        S3_APPS --> S3_CACHE --> S3_SHARDS
        S3_APPS --> S3_QUEUE --> S3_WORKERS
    end
    
    Stage1 --> Stage2 --> Stage3
```

---

## â¬†ï¸ Vertical vs â¡ï¸ Horizontal Scaling

### Visual Comparison

```mermaid
flowchart LR
    subgraph Vertical["â¬†ï¸ Vertical Scaling"]
        direction TB
        V1["ğŸ–¥ï¸ Small\n2 CPU, 4GB"]
        V2["ğŸ–¥ï¸ Medium\n8 CPU, 32GB"]
        V3["ğŸ–¥ï¸ Large\n64 CPU, 256GB"]
        V1 --> V2 --> V3
    end
    
    subgraph Horizontal["â¡ï¸ Horizontal Scaling"]
        direction LR
        H1["ğŸ–¥ï¸ Server 1"]
        H2["ğŸ–¥ï¸ Server 2"]
        H3["ğŸ–¥ï¸ Server 3"]
        H4["ğŸ–¥ï¸ Server N..."]
    end
```

### ğŸ“Š Detailed Comparison

| Aspect | Vertical â¬†ï¸ | Horizontal â¡ï¸ |
|--------|-------------|---------------|
| **How** | Bigger machine | More machines |
| **Cost curve** | ğŸ“ˆ Exponential | ğŸ“‰ Linear |
| **Limit** | Hardware max | â™¾ï¸ Unlimited |
| **Downtime** | Required | Zero (rolling) |
| **Complexity** | ğŸŸ¢ Simple | ğŸ”´ Complex |
| **Data** | Single location | Distributed |
| **Example** | AWS m5.24xlarge | 10x m5.xlarge |

### ğŸ’° Cost Example

```
Vertical Scaling:
  m5.xlarge (4 CPU, 16GB)  = $0.19/hour
  m5.4xlarge (16 CPU, 64GB) = $0.77/hour (4x CPU, 4x cost)
  m5.12xlarge (48 CPU, 192GB) = $2.30/hour (12x CPU, 12x cost)
  
Horizontal Scaling:
  3x m5.xlarge = $0.57/hour for 12 CPU + fault tolerance âœ…
```

---

## ğŸ”‘ Pattern 1: Database Replication

### How It Works

```mermaid
flowchart TB
    subgraph Application
        APP[App Server]
    end
    
    subgraph Database["Database Cluster"]
        MASTER[(Master\nâœï¸ Writes)]
        R1[(Replica 1\nğŸ“– Reads)]
        R2[(Replica 2\nğŸ“– Reads)]
        R3[(Replica 3\nğŸ“– Reads)]
    end
    
    APP -->|"Writes"| MASTER
    MASTER -->|"Replication"| R1 & R2 & R3
    APP -.->|"Reads"| R1 & R2 & R3
```

### Replication Patterns

| Pattern | Write | Read | Consistency | Use Case |
|---------|-------|------|-------------|----------|
| **Master-Slave** | Master only | Slaves | Strong | Most read-heavy apps |
| **Master-Master** | Any master | Any | Eventual | Multi-region |
| **Synchronous** | Wait for all | Any | Strong | Financial data |
| **Asynchronous** | Don't wait | Any | Eventual | Social media |

### âš ï¸ Replication Lag Problem

```mermaid
sequenceDiagram
    participant User
    participant Master
    participant Replica
    
    User->>Master: Update profile (name = "John")
    Master-->>User: Success!
    User->>Replica: Get profile
    Note over Replica: Still has old data!
    Replica-->>User: name = "Old Name" âŒ
    
    Note over Master,Replica: After replication lag...
    Master->>Replica: Replicate update
    User->>Replica: Get profile
    Replica-->>User: name = "John" âœ…
```

**Solutions:**
1. Read your own writes (sticky sessions)
2. Read from master for critical data
3. Show loading state during lag window

---

## ğŸ”‘ Pattern 2: Database Sharding

### What is Sharding?

```mermaid
flowchart TB
    subgraph Before["âŒ Before: Single DB Bottleneck"]
        DB1[(100M users\nSingle DB\nğŸ’€ Overloaded)]
    end
    
    subgraph After["âœ… After: Sharded DB"]
        S1[(Shard 1\nUsers 1-25M)]
        S2[(Shard 2\nUsers 25-50M)]
        S3[(Shard 3\nUsers 50-75M)]
        S4[(Shard 4\nUsers 75-100M)]
    end
    
    Before --> After
```

### Sharding Strategies Comparison

```mermaid
flowchart TB
    subgraph Range["ğŸ“ Range-Based"]
        RR[User A-H â†’ Shard 1]
        RR2[User I-P â†’ Shard 2]
        RR3[User Q-Z â†’ Shard 3]
    end
    
    subgraph Hash["#ï¸âƒ£ Hash-Based"]
        HH["hash(user_id) % 3"]
        HH1[Result 0 â†’ Shard 1]
        HH2[Result 1 â†’ Shard 2]
        HH3[Result 2 â†’ Shard 3]
    end
    
    subgraph Geo["ğŸŒ Geographic"]
        GG1[India users â†’ Shard India]
        GG2[US users â†’ Shard US]
        GG3[EU users â†’ Shard EU]
    end
```

| Strategy | Pros | Cons | Best For |
|----------|------|------|----------|
| **Range** | Easy to implement, range queries work | Hot spots if uneven | Time-series, A-Z data |
| **Hash** | Even distribution | Range queries need all shards | User data |
| **Geographic** | Low latency per region | Cross-region queries slow | Global apps |
| **Directory** | Flexible assignment | Single point of failure | Complex routing |

### âš ï¸ Sharding Challenges

```mermaid
flowchart LR
    subgraph Challenges["ğŸ˜° Sharding Pain Points"]
        C1["ğŸ”— Join across shards"]
        C2["ğŸ”„ Resharding is hard"]
        C3["ğŸ’³ Transactions across shards"]
        C4["ğŸ” Aggregate queries"]
    end
```

| Challenge | Mitigation |
|-----------|------------|
| Joins across shards | Denormalize data, application-level joins |
| Resharding | Use consistent hashing |
| Transactions | Saga pattern, avoid cross-shard |
| Aggregations | Pre-compute, use analytics DB |

### Consistent Hashing Visualization

```mermaid
flowchart TB
    subgraph Ring["Hash Ring (0-360)"]
        direction TB
        S1["Shard 1\n(at 90Â°)"]
        S2["Shard 2\n(at 180Â°)"]
        S3["Shard 3\n(at 270Â°)"]
    end
    
    subgraph Keys["Key Placement"]
        K1["User A (hash=45Â°) â†’ Shard 1"]
        K2["User B (hash=120Â°) â†’ Shard 2"]
        K3["User C (hash=200Â°) â†’ Shard 3"]
    end
```

---

## ğŸ”‘ Pattern 3: Caching Layers

### Cache Hierarchy

```mermaid
flowchart LR
    subgraph Client["ğŸ‘¤ Client Side"]
        BC[ğŸŒ Browser Cache]
    end
    
    subgraph Edge["ğŸŒ Edge"]
        CDN[ğŸ“¡ CDN Cache]
    end
    
    subgraph Server["ğŸ–¥ï¸ Server Side"]
        GW[ğŸšª API Gateway Cache]
        APP[ğŸ’¾ Application Cache]
        REDIS[ğŸ”´ Redis/Memcached]
        DB[ğŸ“Š DB Query Cache]
    end
    
    BC --> CDN --> GW --> APP --> REDIS --> DB
```

### Cache Strategies Visual

```mermaid
flowchart TB
    subgraph CacheAside["Cache-Aside (Lazy Loading)"]
        CA1[App checks cache]
        CA2{Cache hit?}
        CA3[Return data]
        CA4[Query DB]
        CA5[Update cache]
        
        CA1 --> CA2
        CA2 -->|Yes| CA3
        CA2 -->|No| CA4 --> CA5 --> CA3
    end
    
    subgraph WriteThrough["Write-Through"]
        WT1[App writes data]
        WT2[Write to cache]
        WT3[Write to DB]
        WT4[Return success]
        
        WT1 --> WT2 --> WT3 --> WT4
    end
    
    subgraph WriteBehind["Write-Behind (Write-Back)"]
        WB1[App writes data]
        WB2[Write to cache]
        WB3[Return success immediately]
        WB4[Async write to DB]
        
        WB1 --> WB2 --> WB3
        WB2 -.-> WB4
    end
```

### ğŸ“Š Strategy Comparison

| Strategy | Read Perf | Write Perf | Consistency | Use Case |
|----------|-----------|------------|-------------|----------|
| **Cache-Aside** | â­â­â­ | â­â­ | â­â­ | Read-heavy, can tolerate stale |
| **Write-Through** | â­â­â­ | â­ | â­â­â­ | Need consistency |
| **Write-Behind** | â­â­â­ | â­â­â­ | â­ | Write-heavy, can lose data |
| **Read-Through** | â­â­â­ | â­â­ | â­â­ | Simplify app code |

### Cache Invalidation Patterns

```
"There are only two hard things in Computer Science: 
cache invalidation and naming things."
                                    - Phil Karlton
```

| Pattern | How | When to Use |
|---------|-----|-------------|
| **TTL (Time-to-Live)** | Auto-expire after X seconds | When staleness is acceptable |
| **Write-Invalidate** | Delete cache on write | Strong consistency needed |
| **Write-Update** | Update cache on write | Frequent reads after writes |
| **Event-Based** | Invalidate on event | Complex invalidation rules |

---

## ğŸ”‘ Pattern 4: Async Processing

### The Problem

```mermaid
sequenceDiagram
    participant User
    participant API
    participant Email
    participant DB
    
    User->>API: Place Order
    API->>DB: Save Order (100ms)
    API->>Email: Send Confirmation (2000ms) âŒ
    Note over User: User waiting 2100ms!
    API-->>User: Order Placed
```

### The Solution

```mermaid
sequenceDiagram
    participant User
    participant API
    participant Queue
    participant Worker
    participant Email
    participant DB
    
    User->>API: Place Order
    API->>DB: Save Order (100ms)
    API->>Queue: Queue Email (5ms)
    API-->>User: Order Placed âœ…
    Note over User: User got response in 105ms!
    
    Queue->>Worker: Process Email
    Worker->>Email: Send Confirmation
```

### Message Queue Visualization

```mermaid
flowchart LR
    subgraph Producers["ğŸ“¤ Producers"]
        P1[Order Service]
        P2[User Service]
        P3[Payment Service]
    end
    
    subgraph Queue["ğŸ“® Message Queue"]
        Q1["ğŸ“§ Email Queue"]
        Q2["ğŸ“± Push Queue"]
        Q3["ğŸ“Š Analytics Queue"]
    end
    
    subgraph Consumers["ğŸ“¥ Consumers"]
        C1[Email Worker 1]
        C2[Email Worker 2]
        C3[Push Worker]
        C4[Analytics Worker]
    end
    
    Producers --> Queue --> Consumers
```

### ğŸ“Š Queue vs Pub/Sub

| Aspect | Queue | Pub/Sub |
|--------|-------|---------|
| **Delivery** | One consumer | All subscribers |
| **Use Case** | Task distribution | Event broadcasting |
| **Example** | Email sending | Order placed event |
| **Tools** | SQS, Celery | Kafka, SNS, Redis Pub/Sub |

---

## ğŸ”‘ Pattern 5: CQRS

### Command Query Responsibility Segregation

```mermaid
flowchart TB
    subgraph Traditional["âŒ Traditional: Same Model for Read/Write"]
        T1[API] --> T2[(Single DB)]
    end
    
    subgraph CQRS["âœ… CQRS: Separate Models"]
        direction TB
        
        subgraph Write["âœï¸ Write Side"]
            W1[Commands API]
            W2[(Write DB\nNormalized)]
        end
        
        subgraph Read["ğŸ“– Read Side"]
            R1[Queries API]
            R2[(Read DB\nDenormalized)]
        end
        
        W1 --> W2
        W2 -->|Events| SYNC[Event Bus]
        SYNC --> R2
        R1 --> R2
    end
```

### When to Use CQRS

| âœ… Use CQRS When | âŒ Avoid CQRS When |
|------------------|-------------------|
| Read and write patterns differ greatly | Simple CRUD application |
| Complex queries need optimization | Read/write patterns similar |
| High scalability required | Small team |
| Event sourcing is used | Tight budget/timeline |

---

## ğŸŒ Multi-Region Architecture

### Global Distribution

```mermaid
flowchart TB
    subgraph DNS["ğŸŒ Global DNS (GeoDNS)"]
        GLB[Route users to nearest region]
    end
    
    subgraph US["ğŸ‡ºğŸ‡¸ US Region"]
        US_LB[Load Balancer]
        US_APP[App Cluster]
        US_CACHE[(Redis)]
        US_DB[(Primary DB)]
    end
    
    subgraph EU["ğŸ‡ªğŸ‡º EU Region"]
        EU_LB[Load Balancer]
        EU_APP[App Cluster]
        EU_CACHE[(Redis)]
        EU_DB[(Replica DB)]
    end
    
    subgraph ASIA["ğŸ‡®ğŸ‡³ Asia Region"]
        ASIA_LB[Load Balancer]
        ASIA_APP[App Cluster]
        ASIA_CACHE[(Redis)]
        ASIA_DB[(Replica DB)]
    end
    
    GLB --> US_LB & EU_LB & ASIA_LB
    
    US_DB <-->|"Cross-region\nreplication"| EU_DB & ASIA_DB
```

### Latency Impact

```
ğŸ“ Without Multi-Region (US-only):
  - US User: 50ms âœ…
  - EU User: 150ms ğŸ˜
  - Asia User: 250ms âŒ

ğŸ“ With Multi-Region:
  - US User: 50ms âœ…
  - EU User: 50ms âœ…
  - Asia User: 50ms âœ…
```

---

## ğŸ“ Capacity Planning

### Formula

```
Required Servers = (Peak RPS Ã— Latency in sec) / Concurrent Connections per Server
```

### ğŸ“Š Example Calculation

```mermaid
flowchart LR
    subgraph Input["ğŸ“¥ Inputs"]
        I1["Peak: 100,000 RPS"]
        I2["Latency: 200ms"]
        I3["Connections/server: 10,000"]
    end
    
    subgraph Calc["ğŸ”¢ Calculation"]
        C1["100,000 Ã— 0.2 = 20,000"]
        C2["20,000 / 10,000 = 2"]
    end
    
    subgraph Output["ğŸ“¤ Result"]
        O1["Base: 2 servers"]
        O2["+50% buffer: 3 servers"]
        O3["+Redundancy: 6 servers"]
    end
    
    Input --> Calc --> Output
```

---

## ğŸ“š Summary Cheat Sheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALABILITY CHEAT SHEET                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage 1 (1-1K users):                                          â”‚
â”‚   â†’ Single server, single DB                                   â”‚
â”‚                                                                 â”‚
â”‚ Stage 2 (1K-100K users):                                       â”‚
â”‚   â†’ Load balancer + multiple app servers                       â”‚
â”‚   â†’ Add caching (Redis)                                        â”‚
â”‚   â†’ Add read replicas                                          â”‚
â”‚                                                                 â”‚
â”‚ Stage 3 (100K-1M users):                                       â”‚
â”‚   â†’ CDN for static content                                     â”‚
â”‚   â†’ Message queues for async                                   â”‚
â”‚   â†’ Database sharding                                          â”‚
â”‚   â†’ Microservices                                              â”‚
â”‚                                                                 â”‚
â”‚ Stage 4 (1M+ users):                                           â”‚
â”‚   â†’ Multi-region deployment                                    â”‚
â”‚   â†’ CQRS for complex queries                                   â”‚
â”‚   â†’ Event-driven architecture                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Key Takeaways

| Lesson | Remember |
|--------|----------|
| Start simple | Don't over-engineer from day 1 |
| Scale when needed | Measure first, then optimize |
| Cache everything | Redis is your friend |
| Go async | Don't block users |
| Replicate before sharding | Sharding is hard |

---

*Start simple, scale as needed!* ğŸš€
